{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(['seaborn-v0_8-colorblind', 'seaborn-v0_8-darkgrid'])\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "# Automatically reload your external source code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vgg_nets import VGG4Plus, VGG15, VGG15Plus, VGG15PlusPlus, VGG16Plus, VGG16PlusPlus\n",
    "from vgg_nets import VGG8, VGG8OnOff, VGG8OnOffNoReduction, VGG15PlusPlusOffOn, VGG15PlusPlusOffOn\n",
    "from datasets import get_dataset\n",
    "import datasets\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data\n",
    "x_train, y_train, x_val, y_val, x_test, y_test, classnames = datasets.get_dataset('cifar10', val_prop=0.2)\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Dense layer output(output) shape: [1, 10]\n",
      "dense_block:\n",
      "\tDropout layer output(dense_block/dropout_layer_0) shape: [1, 512]\n",
      "\tDense layer output(dense_block/dense_layer_0) shape: [1, 512]\n",
      "Flatten layer output(flatten) shape: [1, 512]\n",
      "conv_block_5:\n",
      "\tDropout layer output(conv_block_5/dropout_layer_2) shape: [1, 1, 1, 512]\n",
      "\tMaxPool2D layer output(conv_block_5/max_pool_layer_2) shape: [1, 1, 1, 512]\n",
      "\tConv2D layer output(conv_block_5/conv_layer_2) shape: [1, 2, 2, 512]\n",
      "\tConv2D layer output(conv_block_5/conv_layer_1) shape: [1, 2, 2, 512]\n",
      "\tConv2D layer output(conv_block_5/conv_layer_0) shape: [1, 2, 2, 512]\n",
      "conv_block_4:\n",
      "\tDropout layer output(conv_block_4/dropout_layer_2) shape: [1, 2, 2, 512]\n",
      "\tMaxPool2D layer output(conv_block_4/max_pool_layer_2) shape: [1, 2, 2, 512]\n",
      "\tConv2D layer output(conv_block_4/conv_layer_2) shape: [1, 4, 4, 512]\n",
      "\tConv2D layer output(conv_block_4/conv_layer_1) shape: [1, 4, 4, 512]\n",
      "\tConv2D layer output(conv_block_4/conv_layer_0) shape: [1, 4, 4, 512]\n",
      "conv_block_3:\n",
      "\tDropout layer output(conv_block_3/dropout_layer_2) shape: [1, 4, 4, 256]\n",
      "\tMaxPool2D layer output(conv_block_3/max_pool_layer_2) shape: [1, 4, 4, 256]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_2) shape: [1, 8, 8, 256]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_1) shape: [1, 8, 8, 256]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_0) shape: [1, 8, 8, 256]\n",
      "conv_block_2:\n",
      "\tDropout layer output(conv_block_2/dropout_layer_1) shape: [1, 8, 8, 128]\n",
      "\tMaxPool2D layer output(conv_block_2/max_pool_layer_1) shape: [1, 8, 8, 128]\n",
      "\tConv2D layer output(conv_block_2/conv_layer_1) shape: [1, 16, 16, 128]\n",
      "\tConv2D layer output(conv_block_2/conv_layer_0) shape: [1, 16, 16, 128]\n",
      "conv_block_1:\n",
      "\tDropout layer output(conv_block_1/dropout_layer_1) shape: [1, 16, 16, 64]\n",
      "\tMaxPool2D layer output(conv_block_1/max_pool_layer_1) shape: [1, 16, 16, 64]\n",
      "\tConv2D layer output(conv_block_1/conv_layer_1) shape: [1, 32, 32, 64]\n",
      "\tConv2D layer output(conv_block_1/conv_layer_0) shape: [1, 32, 32, 64]\n",
      "---------------------------------------------------------------------------\n",
      "Starting training for VGG15PlusPlus...\n",
      "Epoch 1/100: Train Loss: 2.3351, Val Loss: 2.3031, Val Acc: 0.0978, Time: 73.27s\n",
      "Epoch 2/100: Train Loss: 2.3029, Val Loss: 2.3027, Val Acc: 0.0997, Time: 69.07s\n",
      "Epoch 3/100: Train Loss: 2.3027, Val Loss: 2.3027, Val Acc: 0.0997, Time: 68.83s\n",
      "Epoch 4/100: Train Loss: 2.3027, Val Loss: 2.3027, Val Acc: 0.0951, Time: 69.22s\n",
      "Epoch 5/100: Train Loss: 2.3027, Val Loss: 2.3028, Val Acc: 0.0997, Time: 69.63s\n",
      "Epoch 6/100: Train Loss: 2.3026, Val Loss: 2.3030, Val Acc: 0.0997, Time: 69.07s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training for VGG15PlusPlus...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m train_loss_hist, val_loss_hist, val_acc_hist, epochs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_decay_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_max_decays\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\n\u001b[0;32m     23\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m training_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_time\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m minutes)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\alexl\\Documents\\GitHub\\Jan2025OWLyon\\CS444-project-1\\network.py:452\u001b[0m, in \u001b[0;36mDeepNetwork.fit\u001b[1;34m(self, x, y, x_val, y_val, batch_size, max_epochs, val_every, verbose, patience, lr_patience, lr_decay_factor, lr_max_decays)\u001b[0m\n\u001b[0;32m    449\u001b[0m     y_batch \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mgather(y, batch_indices)\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;66;03m# Perform one training step (forward + backward) on this mini-batch\u001b[39;00m\n\u001b[1;32m--> 452\u001b[0m     loss_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m     epoch_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(loss_value)\n\u001b[0;32m    455\u001b[0m \u001b[38;5;66;03m# Average training loss for this epoch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alexl\\miniconda3\\envs\\tf_new\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\alexl\\miniconda3\\envs\\tf_new\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\alexl\\miniconda3\\envs\\tf_new\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:869\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    868\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 869\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    875\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\alexl\\miniconda3\\envs\\tf_new\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alexl\\miniconda3\\envs\\tf_new\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\alexl\\miniconda3\\envs\\tf_new\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\alexl\\miniconda3\\envs\\tf_new\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\alexl\\miniconda3\\envs\\tf_new\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1698\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\alexl\\miniconda3\\envs\\tf_new\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "SEED = 7\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = VGG15PlusPlus(C=10, input_feats_shape=(32, 32, 3), wt_init='he')\n",
    "\n",
    "model.compile(optimizer='adamw')\n",
    "\n",
    "# train the model\n",
    "start_time = time.time()\n",
    "print(f\"Starting training for VGG15PlusPlus...\")\n",
    "\n",
    "train_loss_hist, val_loss_hist, val_acc_hist, epochs = model.fit(\n",
    "    x_train, y_train, \n",
    "    x_val, y_val, \n",
    "    max_epochs=100,\n",
    "    patience=15,\n",
    "    lr_patience=4,\n",
    "    verbose=True,\n",
    "    lr_decay_factor=0.5,\n",
    "    lr_max_decays=12\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
    "\n",
    "# evaluate on test set\n",
    "test_acc, test_loss = model.evaluate(x_test, y_test)\n",
    "print(f\"{model_type} Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "result = {\n",
    "    'model_type': model_type,\n",
    "    'test_accuracy': test_acc,\n",
    "    'test_loss': test_loss,\n",
    "    'train_loss_history': train_loss_hist,\n",
    "    'val_loss_history': val_loss_hist,\n",
    "    'val_acc_history': val_acc_hist,\n",
    "    'epochs': epochs,\n",
    "    'training_time': training_time\n",
    "}\n",
    "results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Training VGG4Plus ====================\n",
      "---------------------------------------------------------------------------\n",
      "Dense layer output(output_layer) shape: [1, 10]\n",
      "Dropout layer output(dropout_layer_3) shape: [1, 128]\n",
      "Dense layer output(dense_layer_3) shape: [1, 128]\n",
      "Flatten layer output(flatten_layer_2) shape: [1, 16384]\n",
      "MaxPool2D layer output(max_pool_layer_1) shape: [1, 16, 16, 64]\n",
      "Conv2D layer output(conv_layer_2) shape: [1, 32, 32, 64]\n",
      "Conv2D layer output(conv_layer_1) shape: [1, 32, 32, 64]\n",
      "---------------------------------------------------------------------------\n",
      "Starting training for VGG4Plus...\n",
      "Epoch 1/100: Train Loss: 1.8623, Val Loss: 1.3538, Val Acc: 0.5233, Time: 10.66s\n",
      "Epoch 2/100: Train Loss: 1.4332, Val Loss: 1.2332, Val Acc: 0.5792, Time: 9.86s\n",
      "Epoch 3/100: Train Loss: 1.2457, Val Loss: 1.0930, Val Acc: 0.6242, Time: 9.83s\n",
      "Epoch 4/100: Train Loss: 1.1278, Val Loss: 1.0274, Val Acc: 0.6477, Time: 9.81s\n",
      "Epoch 5/100: Train Loss: 1.0338, Val Loss: 1.0027, Val Acc: 0.6597, Time: 9.81s\n",
      "Epoch 6/100: Train Loss: 0.9627, Val Loss: 0.9779, Val Acc: 0.6657, Time: 9.85s\n",
      "Epoch 7/100: Train Loss: 0.8982, Val Loss: 0.9841, Val Acc: 0.6671, Time: 9.89s\n",
      "Epoch 8/100: Train Loss: 0.8438, Val Loss: 1.0006, Val Acc: 0.6745, Time: 9.84s\n",
      "Epoch 9/100: Train Loss: 0.7826, Val Loss: 0.9991, Val Acc: 0.6706, Time: 9.84s\n",
      "Current lr= 0.001 Updated lr= 0.0005\n",
      "Epoch 10/100: Train Loss: 0.7017, Val Loss: 1.0216, Val Acc: 0.6752, Time: 9.86s\n",
      "Current lr= 0.0005 Updated lr= 0.00025\n",
      "Epoch 11/100: Train Loss: 0.6457, Val Loss: 1.0251, Val Acc: 0.6809, Time: 9.83s\n",
      "Epoch 12/100: Train Loss: 0.6256, Val Loss: 1.0242, Val Acc: 0.6835, Time: 9.87s\n",
      "Current lr= 0.00025 Updated lr= 0.000125\n",
      "Epoch 13/100: Train Loss: 0.5965, Val Loss: 1.0356, Val Acc: 0.6844, Time: 9.88s\n",
      "Current lr= 0.000125 Updated lr= 6.25e-05\n",
      "Epoch 14/100: Train Loss: 0.5804, Val Loss: 1.0315, Val Acc: 0.6832, Time: 10.00s\n",
      "Epoch 15/100: Train Loss: 0.5769, Val Loss: 1.0393, Val Acc: 0.6872, Time: 10.02s\n",
      "Current lr= 6.25e-05 Updated lr= 3.125e-05\n",
      "Epoch 16/100: Train Loss: 0.5712, Val Loss: 1.0374, Val Acc: 0.6854, Time: 9.92s\n",
      "Epoch 17/100: Train Loss: 0.5633, Val Loss: 1.0449, Val Acc: 0.6870, Time: 9.91s\n",
      "Current lr= 3.125e-05 Updated lr= 1.5625e-05\n",
      "Epoch 18/100: Train Loss: 0.5655, Val Loss: 1.0448, Val Acc: 0.6875, Time: 10.01s\n",
      "Epoch 19/100: Train Loss: 0.5588, Val Loss: 1.0446, Val Acc: 0.6863, Time: 9.81s\n",
      "Current lr= 1.5625e-05 Updated lr= 7.8125e-06\n",
      "Epoch 20/100: Train Loss: 0.5588, Val Loss: 1.0464, Val Acc: 0.6868, Time: 9.78s\n",
      "Finished training after 20 epochs!\n",
      "Training completed in 198.28 seconds (3.30 minutes)\n",
      "VGG4Plus Test Accuracy: 0.6842\n",
      "Saved results for VGG4Plus\n",
      "\n",
      "==================== Training VGG15 ====================\n",
      "---------------------------------------------------------------------------\n",
      "Dense layer output(output) shape: [1, 10]\n",
      "dense_block:\n",
      "\tDropout layer output(dense_block/dropout_layer_0) shape: [1, 512]\n",
      "\tDense layer output(dense_block/dense_layer_0) shape: [1, 512]\n",
      "Flatten layer output(flatten) shape: [1, 512]\n",
      "conv_block_5:\n",
      "\tMaxPool2D layer output(conv_block_5/max_pool_layer_2) shape: [1, 1, 1, 512]\n",
      "\tConv2D layer output(conv_block_5/conv_layer_2) shape: [1, 2, 2, 512]\n",
      "\tConv2D layer output(conv_block_5/conv_layer_1) shape: [1, 2, 2, 512]\n",
      "\tConv2D layer output(conv_block_5/conv_layer_0) shape: [1, 2, 2, 512]\n",
      "conv_block_4:\n",
      "\tMaxPool2D layer output(conv_block_4/max_pool_layer_2) shape: [1, 2, 2, 512]\n",
      "\tConv2D layer output(conv_block_4/conv_layer_2) shape: [1, 4, 4, 512]\n",
      "\tConv2D layer output(conv_block_4/conv_layer_1) shape: [1, 4, 4, 512]\n",
      "\tConv2D layer output(conv_block_4/conv_layer_0) shape: [1, 4, 4, 512]\n",
      "conv_block_3:\n",
      "\tMaxPool2D layer output(conv_block_3/max_pool_layer_2) shape: [1, 4, 4, 256]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_2) shape: [1, 8, 8, 256]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_1) shape: [1, 8, 8, 256]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_0) shape: [1, 8, 8, 256]\n",
      "conv_block_2:\n",
      "\tMaxPool2D layer output(conv_block_2/max_pool_layer_1) shape: [1, 8, 8, 128]\n",
      "\tConv2D layer output(conv_block_2/conv_layer_1) shape: [1, 16, 16, 128]\n",
      "\tConv2D layer output(conv_block_2/conv_layer_0) shape: [1, 16, 16, 128]\n",
      "conv_block_1:\n",
      "\tMaxPool2D layer output(conv_block_1/max_pool_layer_1) shape: [1, 16, 16, 64]\n",
      "\tConv2D layer output(conv_block_1/conv_layer_1) shape: [1, 32, 32, 64]\n",
      "\tConv2D layer output(conv_block_1/conv_layer_0) shape: [1, 32, 32, 64]\n",
      "---------------------------------------------------------------------------\n",
      "Starting training for VGG15...\n",
      "Epoch 1/100: Train Loss: 2.2640, Val Loss: 1.9827, Val Acc: 0.2525, Time: 71.00s\n",
      "Epoch 2/100: Train Loss: 1.7559, Val Loss: 1.6116, Val Acc: 0.3669, Time: 69.40s\n"
     ]
    }
   ],
   "source": [
    "SEED = 1\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "results = []\n",
    "all_models = [\"VGG4Plus\", \"VGG15\", \"VGG15Plus\", \"VGG15PlusPlus\", \"VGG15PlusPlusOffOn\", ]\n",
    "\n",
    "# get dataset with validation split\n",
    "x_train, y_train, x_val, y_val, x_test, y_test, classnames = get_dataset('cifar10', val_prop=0.2)\n",
    "\n",
    "# loop through each model to train\n",
    "for model_idx, model_type in enumerate(all_models):\n",
    "    # clear session to free memory\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    current_seed = SEED + model_idx\n",
    "    tf.random.set_seed(current_seed)\n",
    "    np.random.seed(current_seed)\n",
    "    \n",
    "    print(f\"\\n{'='*20} Training {model_type} {'='*20}\")\n",
    "    \n",
    "    if model_type == \"VGG4Plus\":\n",
    "        model = VGG4Plus(C=10, input_feats_shape=(32, 32, 3), wt_init='he')\n",
    "    elif model_type == \"VGG15\":\n",
    "        model = VGG15(C=10, input_feats_shape=(32, 32, 3), wt_init='he')\n",
    "    elif model_type == \"VGG15Plus\":\n",
    "        model = VGG15Plus(C=10, input_feats_shape=(32, 32, 3), wt_init='he')\n",
    "    elif model_type == \"VGG15PlusPlus\":\n",
    "        model = VGG15PlusPlus(C=10, input_feats_shape=(32, 32, 3), wt_init='he')\n",
    "    elif model_type == \"VGG15PlusPlus\":\n",
    "        model = VGG15PlusPlus(C=10, input_feats_shape=(32, 32, 3), wt_init='he')\n",
    "    elif model_type == \"VGG15PlusPlusOffOn\":\n",
    "        model = VGG15PlusPlusOffOn(C=10, input_feats_shape=(32, 32, 3), wt_init='he')\n",
    "    \n",
    "    # compile with AdamW optimizer\n",
    "    model.compile(optimizer='adamw')\n",
    "    \n",
    "    # train the model\n",
    "    start_time = time.time()\n",
    "    print(f\"Starting training for {model_type}...\")\n",
    "    \n",
    "    train_loss_hist, val_loss_hist, val_acc_hist, epochs = model.fit(\n",
    "        x_train, y_train, \n",
    "        x_val, y_val, \n",
    "        max_epochs=100,\n",
    "        patience=15,\n",
    "        lr_patience=4,\n",
    "        verbose=True,\n",
    "        lr_decay_factor=0.5,\n",
    "        lr_max_decays=12\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
    "    \n",
    "    # evaluate on test set\n",
    "    test_acc, test_loss = model.evaluate(x_test, y_test)\n",
    "    print(f\"{model_type} Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    result = {\n",
    "        'model_type': model_type,\n",
    "        'test_accuracy': test_acc,\n",
    "        'test_loss': test_loss,\n",
    "        'train_loss_history': train_loss_hist,\n",
    "        'val_loss_history': val_loss_hist,\n",
    "        'val_acc_history': val_acc_hist,\n",
    "        'epochs': epochs,\n",
    "        'training_time': training_time\n",
    "    }\n",
    "    results.append(result)\n",
    "    \n",
    "    # Save individual results in case notebook crashes\n",
    "    np.save(f\"{model_type}_results_10b.npy\", result)\n",
    "    print(f\"Saved results for {model_type}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
