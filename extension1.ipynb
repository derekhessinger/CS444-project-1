{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(['seaborn-v0_8-colorblind', 'seaborn-v0_8-darkgrid'])\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "# Automatically reload your external source code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vgg_nets import VGG4Plus, VGG15, VGG15Plus, VGG15PlusPlus, VGG16Plus, VGG16PlusPlus\n",
    "from vgg_nets import VGG8, VGG8OnOff, VGG8OnOffNoReduction, VGG15PlusPlusOffOn, VGG15PlusPlusOffOn, VGG15PlusPlusFullOnOff\n",
    "from datasets import get_dataset\n",
    "import datasets\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Training VGG15PlusPlusOffOn ====================\n",
      "---------------------------------------------------------------------------\n",
      "Dense layer output(output) shape: [1, 10]\n",
      "dense_block:\n",
      "\tDropout layer output(dense_block/dropout_layer_0) shape: [1, 512]\n",
      "\tDense layer output(dense_block/dense_layer_0) shape: [1, 512]\n",
      "Flatten layer output(flatten) shape: [1, 512]\n",
      "conv_block_5:\n",
      "\tDropout layer output(conv_block_5/dropout_layer_2) shape: [1, 1, 1, 512]\n",
      "\tMaxPool2D layer output(conv_block_5/max_pool_layer_2) shape: [1, 1, 1, 512]\n",
      "\tConv2D layer output(conv_block_5/conv_layer_2) shape: [1, 2, 2, 512]\n",
      "\tConv2D layer output(conv_block_5/conv_layer_1) shape: [1, 2, 2, 512]\n",
      "\tConv2D layer output(conv_block_5/conv_layer_0) shape: [1, 2, 2, 512]\n",
      "conv_block_4:\n",
      "\tDropout layer output(conv_block_4/dropout_layer_2) shape: [1, 2, 2, 512]\n",
      "\tMaxPool2D layer output(conv_block_4/max_pool_layer_2) shape: [1, 2, 2, 512]\n",
      "\tConv2D layer output(conv_block_4/conv_layer_2) shape: [1, 4, 4, 512]\n",
      "\tConv2D layer output(conv_block_4/conv_layer_1) shape: [1, 4, 4, 512]\n",
      "\tConv2D layer output(conv_block_4/conv_layer_0) shape: [1, 4, 4, 512]\n",
      "conv_block_3:\n",
      "\tDropout layer output(conv_block_3/dropout_layer_2) shape: [1, 4, 4, 256]\n",
      "\tMaxPool2D layer output(conv_block_3/max_pool_layer_2) shape: [1, 4, 4, 256]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_2) shape: [1, 8, 8, 256]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_1) shape: [1, 8, 8, 256]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_0) shape: [1, 8, 8, 256]\n",
      "conv_block_2:\n",
      "\tDropout layer output(conv_block_2/dropout_layer_1) shape: [1, 8, 8, 128]\n",
      "\tMaxPool2D layer output(conv_block_2/max_pool_layer_1) shape: [1, 8, 8, 128]\n",
      "\tConv2D layer output(conv_block_2/conv_layer_1) shape: [1, 16, 16, 128]\n",
      "\tConv2D layer output(conv_block_2/conv_layer_0) shape: [1, 16, 16, 128]\n",
      "conv_block_1:\n",
      "\tDropout layer output(conv_block_1/dropout_layer_1) shape: [1, 16, 16, 64]\n",
      "\tMaxPool2D layer output(conv_block_1/max_pool_layer_1) shape: [1, 16, 16, 64]\n",
      "\tConv2D layer output(conv_block_1/conv_layer_1) shape: [1, 32, 32, 64]\n",
      "\tConv2D layer output(conv_block_1/conv_layer_0) shape: [1, 32, 32, 64]\n",
      "MaxPool2D layer output(on_off_maxpool) shape: [1, 32, 32, 6]\n",
      "---------------------------------------------------------------------------\n",
      "Starting training for VGG15PlusPlusOffOn...\n"
     ]
    }
   ],
   "source": [
    "SEED = 1\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "results = []\n",
    "all_models = [\"VGG15PlusPlusOffOn\"]\n",
    "# all_models = [\"VGG15PlusPlus\", \"VGG15PlusPlusOffOn\"]\n",
    "# get dataset with validation split\n",
    "x_train, y_train, x_val, y_val, x_test, y_test, classnames = get_dataset('cifar10', val_prop=0.2)\n",
    "\n",
    "# loop through each model to train\n",
    "for model_idx, model_type in enumerate(all_models):\n",
    "    # clear session to free memory\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    current_seed = SEED + model_idx\n",
    "    tf.random.set_seed(current_seed)\n",
    "    np.random.seed(current_seed)\n",
    "    \n",
    "    print(f\"\\n{'='*20} Training {model_type} {'='*20}\")\n",
    "    \n",
    "    if model_type == \"VGG4Plus\":\n",
    "        model = VGG4Plus(C=10, input_feats_shape=(32, 32, 3), wt_init='he')\n",
    "    elif model_type == \"VGG15\":\n",
    "        model = VGG15(C=10, input_feats_shape=(32, 32, 3), wt_init='he')\n",
    "    elif model_type == \"VGG15Plus\":\n",
    "        model = VGG15Plus(C=10, input_feats_shape=(32, 32, 3), wt_init='he')\n",
    "    elif model_type == \"VGG15PlusPlus\":\n",
    "        model = VGG15PlusPlus(C=10, input_feats_shape=(32, 32, 3), wt_init='he')\n",
    "    elif model_type == \"VGG15PlusPlus\":\n",
    "        model = VGG15PlusPlus(C=10, input_feats_shape=(32, 32, 3), wt_init='he')\n",
    "    elif model_type == \"VGG15PlusPlusOffOn\":\n",
    "        model = VGG15PlusPlusOffOn(C=10, input_feats_shape=(32, 32, 3), wt_init='he')\n",
    "    \n",
    "    # compile with AdamW optimizer\n",
    "    model.compile(optimizer='adamw')\n",
    "    \n",
    "    # train the model\n",
    "    start_time = time.time()\n",
    "    print(f\"Starting training for {model_type}...\")\n",
    "    \n",
    "    train_loss_hist, val_loss_hist, val_acc_hist, epochs = model.fit(\n",
    "        x_train, y_train, \n",
    "        x_val, y_val, \n",
    "        max_epochs=100,\n",
    "        patience=15,\n",
    "        lr_patience=4,\n",
    "        verbose=True,\n",
    "        lr_decay_factor=0.5,\n",
    "        lr_max_decays=12\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
    "    \n",
    "    # evaluate on test set\n",
    "    test_acc, test_loss = model.evaluate(x_test, y_test)\n",
    "    print(f\"{model_type} Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    result = {\n",
    "        'model_type': model_type,\n",
    "        'test_accuracy': test_acc,\n",
    "        'test_loss': test_loss,\n",
    "        'train_loss_history': train_loss_hist,\n",
    "        'val_loss_history': val_loss_hist,\n",
    "        'val_acc_history': val_acc_hist,\n",
    "        'epochs': epochs,\n",
    "        'training_time': training_time\n",
    "    }\n",
    "    results.append(result)\n",
    "    \n",
    "    # Save individual results in case notebook crashes\n",
    "    np.save(f\"{model_type}_results_10b.npy\", result)\n",
    "    print(f\"Saved results for {model_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Training VGG15PlusPlus ====================\n",
      "---------------------------------------------------------------------------\n",
      "Dense layer output(output) shape: [1, 10]\n",
      "dense_block:\n",
      "\tDropout layer output(dense_block/dropout_layer_0) shape: [1, 512]\n",
      "\tDense layer output(dense_block/dense_layer_0) shape: [1, 512]\n",
      "Flatten layer output(flatten) shape: [1, 512]\n",
      "conv_block_5:\n",
      "\tDropout layer output(conv_block_5/dropout_layer_2) shape: [1, 1, 1, 512]\n",
      "\tMaxPool2D layer output(conv_block_5/max_pool_layer_2) shape: [1, 1, 1, 512]\n",
      "\tConv2D layer output(conv_block_5/conv_layer_2) shape: [1, 2, 2, 512]\n",
      "\tConv2D layer output(conv_block_5/conv_layer_1) shape: [1, 2, 2, 512]\n",
      "\tConv2D layer output(conv_block_5/conv_layer_0) shape: [1, 2, 2, 512]\n",
      "conv_block_4:\n",
      "\tDropout layer output(conv_block_4/dropout_layer_2) shape: [1, 2, 2, 512]\n",
      "\tMaxPool2D layer output(conv_block_4/max_pool_layer_2) shape: [1, 2, 2, 512]\n",
      "\tConv2D layer output(conv_block_4/conv_layer_2) shape: [1, 4, 4, 512]\n",
      "\tConv2D layer output(conv_block_4/conv_layer_1) shape: [1, 4, 4, 512]\n",
      "\tConv2D layer output(conv_block_4/conv_layer_0) shape: [1, 4, 4, 512]\n",
      "conv_block_3:\n",
      "\tDropout layer output(conv_block_3/dropout_layer_2) shape: [1, 4, 4, 256]\n",
      "\tMaxPool2D layer output(conv_block_3/max_pool_layer_2) shape: [1, 4, 4, 256]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_2) shape: [1, 8, 8, 256]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_1) shape: [1, 8, 8, 256]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_0) shape: [1, 8, 8, 256]\n",
      "conv_block_2:\n",
      "\tDropout layer output(conv_block_2/dropout_layer_1) shape: [1, 8, 8, 128]\n",
      "\tMaxPool2D layer output(conv_block_2/max_pool_layer_1) shape: [1, 8, 8, 128]\n",
      "\tConv2D layer output(conv_block_2/conv_layer_1) shape: [1, 16, 16, 128]\n",
      "\tConv2D layer output(conv_block_2/conv_layer_0) shape: [1, 16, 16, 128]\n",
      "conv_block_1:\n",
      "\tDropout layer output(conv_block_1/dropout_layer_1) shape: [1, 16, 16, 64]\n",
      "\tMaxPool2D layer output(conv_block_1/max_pool_layer_1) shape: [1, 16, 16, 64]\n",
      "\tConv2D layer output(conv_block_1/conv_layer_1) shape: [1, 32, 32, 64]\n",
      "\tConv2D layer output(conv_block_1/conv_layer_0) shape: [1, 32, 32, 64]\n",
      "---------------------------------------------------------------------------\n",
      "Starting training for VGG15PlusPlus...\n",
      "Epoch 1/100: Train Loss: 2.1382, Val Loss: 1.9001, Val Acc: 0.1923, Time: 72.50s\n",
      "Epoch 2/100: Train Loss: 1.8708, Val Loss: 1.7480, Val Acc: 0.3016, Time: 70.28s\n",
      "Epoch 3/100: Train Loss: 1.6432, Val Loss: 1.4899, Val Acc: 0.3878, Time: 69.96s\n",
      "Epoch 4/100: Train Loss: 1.4719, Val Loss: 1.3464, Val Acc: 0.4864, Time: 69.91s\n",
      "Epoch 5/100: Train Loss: 1.3090, Val Loss: 1.2662, Val Acc: 0.5428, Time: 69.86s\n",
      "Epoch 6/100: Train Loss: 1.1801, Val Loss: 1.0155, Val Acc: 0.6508, Time: 69.81s\n",
      "Epoch 7/100: Train Loss: 1.0529, Val Loss: 0.9476, Val Acc: 0.6736, Time: 69.82s\n",
      "Epoch 8/100: Train Loss: 0.9966, Val Loss: 0.9229, Val Acc: 0.6820, Time: 69.76s\n",
      "Epoch 9/100: Train Loss: 0.9111, Val Loss: 0.8773, Val Acc: 0.6946, Time: 69.69s\n",
      "Epoch 10/100: Train Loss: 0.8555, Val Loss: 0.8014, Val Acc: 0.7357, Time: 69.78s\n",
      "Epoch 11/100: Train Loss: 0.8344, Val Loss: 0.7555, Val Acc: 0.7468, Time: 69.64s\n",
      "Epoch 12/100: Train Loss: 0.8174, Val Loss: 0.7346, Val Acc: 0.7556, Time: 69.57s\n",
      "Epoch 13/100: Train Loss: 0.7748, Val Loss: 0.7016, Val Acc: 0.7637, Time: 69.64s\n",
      "Epoch 14/100: Train Loss: 0.7585, Val Loss: 0.7634, Val Acc: 0.7427, Time: 69.67s\n",
      "Epoch 15/100: Train Loss: 0.7496, Val Loss: 0.7639, Val Acc: 0.7522, Time: 69.53s\n",
      "Epoch 16/100: Train Loss: 0.7491, Val Loss: 0.7200, Val Acc: 0.7691, Time: 69.70s\n",
      "Current lr= 0.001 Updated lr= 0.0005\n",
      "Epoch 17/100: Train Loss: 0.5707, Val Loss: 0.6294, Val Acc: 0.7967, Time: 69.61s\n",
      "Epoch 18/100: Train Loss: 0.5413, Val Loss: 0.6022, Val Acc: 0.8047, Time: 69.48s\n",
      "Epoch 19/100: Train Loss: 0.5353, Val Loss: 0.5753, Val Acc: 0.8108, Time: 69.64s\n",
      "Epoch 20/100: Train Loss: 0.5227, Val Loss: 0.5675, Val Acc: 0.8191, Time: 69.58s\n",
      "Epoch 21/100: Train Loss: 0.5055, Val Loss: 0.5527, Val Acc: 0.8139, Time: 69.64s\n",
      "Epoch 22/100: Train Loss: 0.5147, Val Loss: 0.5565, Val Acc: 0.8181, Time: 69.55s\n",
      "Epoch 23/100: Train Loss: 0.4943, Val Loss: 0.6127, Val Acc: 0.8046, Time: 69.58s\n",
      "Epoch 24/100: Train Loss: 0.4835, Val Loss: 0.5305, Val Acc: 0.8299, Time: 69.50s\n",
      "Epoch 25/100: Train Loss: 0.4754, Val Loss: 0.5431, Val Acc: 0.8214, Time: 69.59s\n",
      "Epoch 26/100: Train Loss: 0.4634, Val Loss: 0.5437, Val Acc: 0.8227, Time: 69.60s\n",
      "Epoch 27/100: Train Loss: 0.4607, Val Loss: 0.5564, Val Acc: 0.8227, Time: 69.56s\n",
      "Current lr= 0.0005 Updated lr= 0.00025\n",
      "Epoch 28/100: Train Loss: 0.3720, Val Loss: 0.5110, Val Acc: 0.8416, Time: 69.58s\n",
      "Epoch 29/100: Train Loss: 0.3412, Val Loss: 0.4750, Val Acc: 0.8478, Time: 69.47s\n",
      "Epoch 30/100: Train Loss: 0.3415, Val Loss: 0.4775, Val Acc: 0.8477, Time: 69.66s\n",
      "Epoch 31/100: Train Loss: 0.3295, Val Loss: 0.4742, Val Acc: 0.8496, Time: 69.60s\n",
      "Epoch 32/100: Train Loss: 0.3199, Val Loss: 0.4994, Val Acc: 0.8466, Time: 69.63s\n",
      "Epoch 33/100: Train Loss: 0.3195, Val Loss: 0.5137, Val Acc: 0.8456, Time: 69.68s\n",
      "Epoch 34/100: Train Loss: 0.3058, Val Loss: 0.4883, Val Acc: 0.8452, Time: 69.62s\n",
      "Current lr= 0.00025 Updated lr= 0.000125\n",
      "Epoch 35/100: Train Loss: 0.2575, Val Loss: 0.4700, Val Acc: 0.8601, Time: 69.64s\n",
      "Epoch 36/100: Train Loss: 0.2360, Val Loss: 0.4683, Val Acc: 0.8607, Time: 69.60s\n",
      "Epoch 37/100: Train Loss: 0.2323, Val Loss: 0.4819, Val Acc: 0.8564, Time: 69.53s\n",
      "Epoch 38/100: Train Loss: 0.2275, Val Loss: 0.4863, Val Acc: 0.8587, Time: 69.60s\n",
      "Epoch 39/100: Train Loss: 0.2166, Val Loss: 0.5190, Val Acc: 0.8513, Time: 69.67s\n",
      "Current lr= 0.000125 Updated lr= 6.25e-05\n",
      "Epoch 40/100: Train Loss: 0.1832, Val Loss: 0.4970, Val Acc: 0.8604, Time: 69.58s\n",
      "Current lr= 6.25e-05 Updated lr= 3.125e-05\n",
      "Epoch 41/100: Train Loss: 0.1697, Val Loss: 0.4574, Val Acc: 0.8677, Time: 69.78s\n",
      "Epoch 42/100: Train Loss: 0.1626, Val Loss: 0.4586, Val Acc: 0.8677, Time: 69.60s\n",
      "Epoch 43/100: Train Loss: 0.1541, Val Loss: 0.4617, Val Acc: 0.8673, Time: 69.64s\n",
      "Epoch 44/100: Train Loss: 0.1536, Val Loss: 0.4607, Val Acc: 0.8688, Time: 69.72s\n",
      "Current lr= 3.125e-05 Updated lr= 1.5625e-05\n",
      "Epoch 45/100: Train Loss: 0.1446, Val Loss: 0.4638, Val Acc: 0.8687, Time: 69.66s\n",
      "Current lr= 1.5625e-05 Updated lr= 7.8125e-06\n",
      "Epoch 46/100: Train Loss: 0.1374, Val Loss: 0.4650, Val Acc: 0.8711, Time: 69.56s\n",
      "Epoch 47/100: Train Loss: 0.1372, Val Loss: 0.4691, Val Acc: 0.8701, Time: 69.67s\n",
      "Current lr= 7.8125e-06 Updated lr= 3.90625e-06\n",
      "Epoch 48/100: Train Loss: 0.1409, Val Loss: 0.4678, Val Acc: 0.8685, Time: 69.67s\n",
      "Current lr= 3.90625e-06 Updated lr= 1.953125e-06\n",
      "Epoch 49/100: Train Loss: 0.1357, Val Loss: 0.4640, Val Acc: 0.8687, Time: 69.71s\n",
      "Epoch 50/100: Train Loss: 0.1300, Val Loss: 0.4659, Val Acc: 0.8682, Time: 69.59s\n",
      "Epoch 51/100: Train Loss: 0.1351, Val Loss: 0.4659, Val Acc: 0.8687, Time: 69.69s\n",
      "Epoch 52/100: Train Loss: 0.1331, Val Loss: 0.4660, Val Acc: 0.8685, Time: 69.74s\n",
      "Current lr= 1.953125e-06 Updated lr= 9.765625e-07\n",
      "Epoch 53/100: Train Loss: 0.1353, Val Loss: 0.4649, Val Acc: 0.8679, Time: 69.78s\n",
      "Epoch 54/100: Train Loss: 0.1389, Val Loss: 0.4633, Val Acc: 0.8680, Time: 69.68s\n",
      "Epoch 55/100: Train Loss: 0.1282, Val Loss: 0.4643, Val Acc: 0.8689, Time: 69.60s\n",
      "Finished training after 55 epochs!\n",
      "Training completed in 3834.41 seconds (63.91 minutes)\n",
      "VGG15PlusPlus Test Accuracy: 0.8683\n",
      "Saved results for VGG15PlusPlus\n"
     ]
    }
   ],
   "source": [
    "SEED = 1\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "results = []\n",
    "all_models = [\"VGG15PlusPlus\"]\n",
    "# all_models = [\"VGG15PlusPlus\", \"VGG15PlusPlusOffOn\"]\n",
    "# get dataset with validation split\n",
    "x_train, y_train, x_val, y_val, x_test, y_test, classnames = get_dataset('cifar10', val_prop=0.2)\n",
    "\n",
    "# loop through each model to train\n",
    "for model_idx, model_type in enumerate(all_models):\n",
    "    # clear session to free memory\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    current_seed = SEED + model_idx\n",
    "    tf.random.set_seed(current_seed)\n",
    "    np.random.seed(current_seed)\n",
    "    \n",
    "    print(f\"\\n{'='*20} Training {model_type} {'='*20}\")\n",
    "    \n",
    "    if model_type == \"VGG4Plus\":\n",
    "        model = VGG4Plus(C=10, input_feats_shape=(32, 32, 3), wt_init='he')\n",
    "    elif model_type == \"VGG15\":\n",
    "        model = VGG15(C=10, input_feats_shape=(32, 32, 3), wt_init='he')\n",
    "    elif model_type == \"VGG15Plus\":\n",
    "        model = VGG15Plus(C=10, input_feats_shape=(32, 32, 3), wt_init='he')\n",
    "    elif model_type == \"VGG15PlusPlus\":\n",
    "        model = VGG15PlusPlus(C=10, input_feats_shape=(32, 32, 3), wt_init='he')\n",
    "    elif model_type == \"VGG15PlusPlus\":\n",
    "        model = VGG15PlusPlus(C=10, input_feats_shape=(32, 32, 3), wt_init='he')\n",
    "    elif model_type == \"VGG15PlusPlusOffOn\":\n",
    "        model = VGG15PlusPlusOffOn(C=10, input_feats_shape=(32, 32, 3), wt_init='he')\n",
    "    \n",
    "    # compile with AdamW optimizer\n",
    "    model.compile(optimizer='adamw')\n",
    "    \n",
    "    # train the model\n",
    "    start_time = time.time()\n",
    "    print(f\"Starting training for {model_type}...\")\n",
    "    \n",
    "    train_loss_hist, val_loss_hist, val_acc_hist, epochs = model.fit(\n",
    "        x_train, y_train, \n",
    "        x_val, y_val, \n",
    "        max_epochs=100,\n",
    "        patience=15,\n",
    "        lr_patience=4,\n",
    "        verbose=True,\n",
    "        lr_decay_factor=0.5,\n",
    "        lr_max_decays=12\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
    "    \n",
    "    # evaluate on test set\n",
    "    test_acc, test_loss = model.evaluate(x_test, y_test)\n",
    "    print(f\"{model_type} Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    result = {\n",
    "        'model_type': model_type,\n",
    "        'test_accuracy': test_acc,\n",
    "        'test_loss': test_loss,\n",
    "        'train_loss_history': train_loss_hist,\n",
    "        'val_loss_history': val_loss_hist,\n",
    "        'val_acc_history': val_acc_hist,\n",
    "        'epochs': epochs,\n",
    "        'training_time': training_time\n",
    "    }\n",
    "    results.append(result)\n",
    "    \n",
    "    # Save individual results in case notebook crashes\n",
    "    np.save(f\"{model_type}_results_10b.npy\", result)\n",
    "    print(f\"Saved results for {model_type}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OnOFF Whole Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\alexl\\miniconda3\\envs\\tf_new\\lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "\n",
      "==================== Training VGG15PlusPlusFullOnOff ====================\n",
      "---------------------------------------------------------------------------\n",
      "Dense layer output(output) shape: [1, 10]\n",
      "dense_block:\n",
      "\tDropout layer output(dense_block/dropout_layer_0) shape: [1, 512]\n",
      "\tDense layer output(dense_block/dense_layer_0) shape: [1, 512]\n",
      "Flatten layer output(flatten) shape: [1, 1024]\n",
      "conv_block_5:\n",
      "\tDropout layer output(conv_block_5/dropout_layer_2) shape: [1, 1, 1, 1024]\n",
      "\tMaxPool2D layer output(conv_block_5/max_pool_on_off_layer_2) shape: [1, 1, 1, 1024]\n",
      "\tConv2D layer output(conv_block_5/conv_layer_2) shape: [1, 2, 2, 512]\n",
      "\tConv2D layer output(conv_block_5/conv_layer_1) shape: [1, 2, 2, 512]\n",
      "\tConv2D layer output(conv_block_5/conv_layer_0) shape: [1, 2, 2, 512]\n",
      "conv_block_4:\n",
      "\tDropout layer output(conv_block_4/dropout_layer_2) shape: [1, 2, 2, 1024]\n",
      "\tMaxPool2D layer output(conv_block_4/max_pool_on_off_layer_2) shape: [1, 2, 2, 1024]\n",
      "\tConv2D layer output(conv_block_4/conv_layer_2) shape: [1, 4, 4, 512]\n",
      "\tConv2D layer output(conv_block_4/conv_layer_1) shape: [1, 4, 4, 512]\n",
      "\tConv2D layer output(conv_block_4/conv_layer_0) shape: [1, 4, 4, 512]\n",
      "conv_block_3:\n",
      "\tDropout layer output(conv_block_3/dropout_layer_2) shape: [1, 4, 4, 512]\n",
      "\tMaxPool2D layer output(conv_block_3/max_pool_on_off_layer_2) shape: [1, 4, 4, 512]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_2) shape: [1, 8, 8, 256]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_1) shape: [1, 8, 8, 256]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_0) shape: [1, 8, 8, 256]\n",
      "conv_block_2:\n",
      "\tDropout layer output(conv_block_2/dropout_layer_1) shape: [1, 8, 8, 256]\n",
      "\tMaxPool2D layer output(conv_block_2/max_pool_on_off_layer_1) shape: [1, 8, 8, 256]\n",
      "\tConv2D layer output(conv_block_2/conv_layer_1) shape: [1, 16, 16, 128]\n",
      "\tConv2D layer output(conv_block_2/conv_layer_0) shape: [1, 16, 16, 128]\n",
      "conv_block_1:\n",
      "\tDropout layer output(conv_block_1/dropout_layer_1) shape: [1, 16, 16, 128]\n",
      "\tMaxPool2D layer output(conv_block_1/max_pool_on_off_layer_1) shape: [1, 16, 16, 128]\n",
      "\tConv2D layer output(conv_block_1/conv_layer_1) shape: [1, 32, 32, 64]\n",
      "\tConv2D layer output(conv_block_1/conv_layer_0) shape: [1, 32, 32, 64]\n",
      "---------------------------------------------------------------------------\n",
      "Starting training for VGG15PlusPlusFullOnOff...\n",
      "Epoch 1/100: Train Loss: 2.3059, Val Loss: 2.3029, Val Acc: 0.0978, Time: 88.40s\n",
      "Epoch 2/100: Train Loss: 2.3028, Val Loss: 2.3026, Val Acc: 0.1012, Time: 84.22s\n",
      "Epoch 3/100: Train Loss: 2.3027, Val Loss: 2.3027, Val Acc: 0.0997, Time: 84.25s\n",
      "Epoch 4/100: Train Loss: 2.3027, Val Loss: 2.3027, Val Acc: 0.0951, Time: 85.32s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 44\u001b[0m train_loss_hist, val_loss_hist, val_acc_hist, epochs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_decay_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_max_decays\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\n\u001b[0;32m     53\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m training_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_time\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m minutes)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\alexl\\Documents\\GitHub\\Jan2025OWLyon\\CS444-project-1\\network.py:452\u001b[0m, in \u001b[0;36mDeepNetwork.fit\u001b[1;34m(self, x, y, x_val, y_val, batch_size, max_epochs, val_every, verbose, patience, lr_patience, lr_decay_factor, lr_max_decays)\u001b[0m\n\u001b[0;32m    449\u001b[0m     y_batch \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mgather(y, batch_indices)\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;66;03m# Perform one training step (forward + backward) on this mini-batch\u001b[39;00m\n\u001b[1;32m--> 452\u001b[0m     loss_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m     epoch_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(loss_value)\n\u001b[0;32m    455\u001b[0m \u001b[38;5;66;03m# Average training loss for this epoch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alexl\\miniconda3\\envs\\tf_new\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\alexl\\miniconda3\\envs\\tf_new\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\alexl\\miniconda3\\envs\\tf_new\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:869\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    868\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 869\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    875\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\alexl\\miniconda3\\envs\\tf_new\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alexl\\miniconda3\\envs\\tf_new\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\alexl\\miniconda3\\envs\\tf_new\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\alexl\\miniconda3\\envs\\tf_new\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\alexl\\miniconda3\\envs\\tf_new\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1698\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\alexl\\miniconda3\\envs\\tf_new\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "SEED = 1\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "results = []\n",
    "all_models = [\"VGG15PlusPlusFullOnOff\"]\n",
    "# all_models = [\"VGG15PlusPlus\", \"VGG15PlusPlusOffOn\"]\n",
    "# get dataset with validation split\n",
    "x_train, y_train, x_val, y_val, x_test, y_test, classnames = get_dataset('cifar10', val_prop=0.2)\n",
    "\n",
    "# loop through each model to train\n",
    "for model_idx, model_type in enumerate(all_models):\n",
    "    # clear session to free memory\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    current_seed = SEED + model_idx\n",
    "    tf.random.set_seed(current_seed)\n",
    "    np.random.seed(current_seed)\n",
    "    \n",
    "    print(f\"\\n{'='*20} Training {model_type} {'='*20}\")\n",
    "    \n",
    "    if model_type == \"VGG4Plus\":\n",
    "        model = VGG4Plus(C=10, input_feats_shape=(32, 32, 3), wt_init='he')\n",
    "    elif model_type == \"VGG15\":\n",
    "        model = VGG15(C=10, input_feats_shape=(32, 32, 3), wt_init='he')\n",
    "    elif model_type == \"VGG15Plus\":\n",
    "        model = VGG15Plus(C=10, input_feats_shape=(32, 32, 3), wt_init='he')\n",
    "    elif model_type == \"VGG15PlusPlus\":\n",
    "        model = VGG15PlusPlus(C=10, input_feats_shape=(32, 32, 3), wt_init='he')\n",
    "    elif model_type == \"VGG15PlusPlus\":\n",
    "        model = VGG15PlusPlus(C=10, input_feats_shape=(32, 32, 3), wt_init='he')\n",
    "    elif model_type == \"VGG15PlusPlusOffOn\":\n",
    "        model = VGG15PlusPlusOffOn(C=10, input_feats_shape=(32, 32, 3), wt_init='he')\n",
    "    elif model_type == \"VGG15PlusPlusFullOnOff\":\n",
    "        model = VGG15PlusPlusFullOnOff(C=10, input_feats_shape=(32, 32, 3), wt_init='he')\n",
    "    \n",
    "    # compile with AdamW optimizer\n",
    "    model.compile(optimizer='adamw')\n",
    "    \n",
    "    # train the model\n",
    "    start_time = time.time()\n",
    "    print(f\"Starting training for {model_type}...\")\n",
    "    \n",
    "    train_loss_hist, val_loss_hist, val_acc_hist, epochs = model.fit(\n",
    "        x_train, y_train, \n",
    "        x_val, y_val, \n",
    "        max_epochs=100,\n",
    "        patience=15,\n",
    "        lr_patience=4,\n",
    "        verbose=True,\n",
    "        lr_decay_factor=0.5,\n",
    "        lr_max_decays=12\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
    "    \n",
    "    # evaluate on test set\n",
    "    test_acc, test_loss = model.evaluate(x_test, y_test)\n",
    "    print(f\"{model_type} Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    result = {\n",
    "        'model_type': model_type,\n",
    "        'test_accuracy': test_acc,\n",
    "        'test_loss': test_loss,\n",
    "        'train_loss_history': train_loss_hist,\n",
    "        'val_loss_history': val_loss_hist,\n",
    "        'val_acc_history': val_acc_hist,\n",
    "        'epochs': epochs,\n",
    "        'training_time': training_time\n",
    "    }\n",
    "    results.append(result)\n",
    "    \n",
    "    # Save individual results in case notebook crashes\n",
    "    np.save(f\"{model_type}_results_10b.npy\", result)\n",
    "    print(f\"Saved results for {model_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the saved result dictionaries.\n",
    "results_model1 = np.load(\"model_type1_results_10b.npy\", allow_pickle=True).item()\n",
    "results_model2 = np.load(\"model_type2_results_10b.npy\", allow_pickle=True).item()\n",
    "\n",
    "# Extract training and validation loss histories.\n",
    "train_loss1 = results_model1['train_loss_history']\n",
    "val_loss1 = results_model1['val_loss_history']\n",
    "\n",
    "train_loss2 = results_model2['train_loss_history']\n",
    "val_loss2 = results_model2['val_loss_history']\n",
    "\n",
    "# Determine the number of epochs for each model.\n",
    "epochs1 = range(1, len(train_loss1) + 1)\n",
    "epochs2 = range(1, len(train_loss2) + 1)\n",
    "\n",
    "# Plot the loss curves.\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs1, train_loss1, label=\"Model 1 Train Loss\", color=\"blue\")\n",
    "plt.plot(epochs1, val_loss1, label=\"Model 1 Val Loss\", color=\"cyan\")\n",
    "plt.plot(epochs2, train_loss2, label=\"Model 2 Train Loss\", color=\"red\")\n",
    "plt.plot(epochs2, val_loss2, label=\"Model 2 Val Loss\", color=\"orange\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Comparison\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
