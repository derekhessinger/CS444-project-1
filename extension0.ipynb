{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vgg16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.digitalocean.com/community/tutorials/popular-deep-learning-architectures-alexnet-vgg-googlenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(['seaborn-v0_8-colorblind', 'seaborn-v0_8-darkgrid'])\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "# Automatically reload your external source code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vgg_nets import VGG16Plus, VGG16PlusPlus\n",
    "from vgg_nets import VGG8\n",
    "import datasets\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data\n",
    "x_train, y_train, x_val, y_val, x_test, y_test, classnames = datasets.get_dataset('cifar10', val_prop=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Training VGG16Plus ====================\n",
      "VGG16Plus\n",
      "---------------------------------------------------------------------------\n",
      "Dense layer output(output) shape: [1, 10]\n",
      "dense_block:\n",
      "\tDropout layer output(dense_block/dropout_layer_1) shape: [1, 4096]\n",
      "\tDense layer output(dense_block/dense_layer_1) shape: [1, 4096]\n",
      "\tDropout layer output(dense_block/dropout_layer_0) shape: [1, 4096]\n",
      "\tDense layer output(dense_block/dense_layer_0) shape: [1, 4096]\n",
      "Flatten layer output(flatten) shape: [1, 512]\n",
      "conv_block_5:\n",
      "\tMaxPool2D layer output(conv_block_5/max_pool_layer_2) shape: [1, 1, 1, 512]\n",
      "\tConv2D layer output(conv_block_5/conv_layer_2) shape: [1, 2, 2, 512]\n",
      "\tConv2D layer output(conv_block_5/conv_layer_1) shape: [1, 2, 2, 512]\n",
      "\tConv2D layer output(conv_block_5/conv_layer_0) shape: [1, 2, 2, 512]\n",
      "conv_block_4:\n",
      "\tMaxPool2D layer output(conv_block_4/max_pool_layer_2) shape: [1, 2, 2, 512]\n",
      "\tConv2D layer output(conv_block_4/conv_layer_2) shape: [1, 4, 4, 512]\n",
      "\tConv2D layer output(conv_block_4/conv_layer_1) shape: [1, 4, 4, 512]\n",
      "\tConv2D layer output(conv_block_4/conv_layer_0) shape: [1, 4, 4, 512]\n",
      "conv_block_3:\n",
      "\tMaxPool2D layer output(conv_block_3/max_pool_layer_2) shape: [1, 4, 4, 256]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_2) shape: [1, 8, 8, 256]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_1) shape: [1, 8, 8, 256]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_0) shape: [1, 8, 8, 256]\n",
      "conv_block_2:\n",
      "\tMaxPool2D layer output(conv_block_2/max_pool_layer_1) shape: [1, 8, 8, 128]\n",
      "\tConv2D layer output(conv_block_2/conv_layer_1) shape: [1, 16, 16, 128]\n",
      "\tConv2D layer output(conv_block_2/conv_layer_0) shape: [1, 16, 16, 128]\n",
      "conv_block_1:\n",
      "\tMaxPool2D layer output(conv_block_1/max_pool_layer_1) shape: [1, 16, 16, 64]\n",
      "\tConv2D layer output(conv_block_1/conv_layer_1) shape: [1, 32, 32, 64]\n",
      "\tConv2D layer output(conv_block_1/conv_layer_0) shape: [1, 32, 32, 64]\n",
      "---------------------------------------------------------------------------\n",
      "Starting training for VGG16Plus...\n",
      "Epoch 1/100: Train Loss: 2.3397, Val Loss: 2.3029, Val Acc: 0.0978, Time: 83.78s\n",
      "Epoch 2/100: Train Loss: 2.3028, Val Loss: 2.3026, Val Acc: 0.1012, Time: 80.60s\n",
      "Epoch 3/100: Train Loss: 2.3027, Val Loss: 2.3028, Val Acc: 0.0997, Time: 80.08s\n",
      "Epoch 4/100: Train Loss: 2.3027, Val Loss: 2.3027, Val Acc: 0.0951, Time: 80.35s\n",
      "Epoch 5/100: Train Loss: 2.3027, Val Loss: 2.3028, Val Acc: 0.0997, Time: 80.15s\n",
      "Current lr= 0.001 Updated lr= 0.0005\n",
      "Epoch 6/100: Train Loss: 2.3025, Val Loss: 2.3029, Val Acc: 0.0997, Time: 80.43s\n",
      "Epoch 7/100: Train Loss: 2.3026, Val Loss: 2.3028, Val Acc: 0.0951, Time: 80.39s\n",
      "Current lr= 0.0005 Updated lr= 0.00025\n",
      "Epoch 8/100: Train Loss: 2.3026, Val Loss: 2.3028, Val Acc: 0.0951, Time: 80.09s\n",
      "Current lr= 0.00025 Updated lr= 0.000125\n",
      "Epoch 9/100: Train Loss: 2.3026, Val Loss: 2.3028, Val Acc: 0.0951, Time: 80.08s\n",
      "Epoch 10/100: Train Loss: 2.3025, Val Loss: 2.3028, Val Acc: 0.0951, Time: 80.09s\n",
      "Epoch 11/100: Train Loss: 2.3026, Val Loss: 2.3027, Val Acc: 0.0951, Time: 80.17s\n",
      "Epoch 12/100: Train Loss: 2.3026, Val Loss: 2.3027, Val Acc: 0.0951, Time: 80.30s\n",
      "Epoch 13/100: Train Loss: 2.3026, Val Loss: 2.3028, Val Acc: 0.0951, Time: 80.21s\n",
      "Epoch 14/100: Train Loss: 2.3026, Val Loss: 2.3027, Val Acc: 0.0951, Time: 80.09s\n",
      "Epoch 15/100: Train Loss: 2.3026, Val Loss: 2.3027, Val Acc: 0.0951, Time: 80.21s\n",
      "Epoch 16/100: Train Loss: 2.3026, Val Loss: 2.3027, Val Acc: 0.0951, Time: 80.18s\n",
      "Finished training after 16 epochs!\n",
      "Training completed in 1287.21 seconds (21.45 minutes)\n",
      "VGG16Plus Test Accuracy: 0.1001\n",
      "Saved results for VGG16Plus\n",
      "\n",
      "==================== Training VGG16PlusPlus ====================\n",
      "VGG16PlusPlus\n",
      "---------------------------------------------------------------------------\n",
      "Dense layer output(output) shape: [1, 10]\n",
      "dense_block_1:\n",
      "\tDropout layer output(dense_block_1/dropout_layer_1) shape: [1, 4096]\n",
      "\tDense layer output(dense_block_1/dense_layer_1) shape: [1, 4096]\n",
      "\tDropout layer output(dense_block_1/dropout_layer_0) shape: [1, 4096]\n",
      "\tDense layer output(dense_block_1/dense_layer_0) shape: [1, 4096]\n",
      "Flatten layer output(flatten) shape: [1, 512]\n",
      "conv_block_5:\n",
      "\tDropout layer output(conv_block_5/dropout_layer_2) shape: [1, 1, 1, 512]\n",
      "\tMaxPool2D layer output(conv_block_5/max_pool_layer_2) shape: [1, 1, 1, 512]\n",
      "\tConv2D layer output(conv_block_5/conv_layer_2) shape: [1, 2, 2, 512]\n",
      "\tConv2D layer output(conv_block_5/conv_layer_1) shape: [1, 2, 2, 512]\n",
      "\tConv2D layer output(conv_block_5/conv_layer_0) shape: [1, 2, 2, 512]\n",
      "conv_block_4:\n",
      "\tDropout layer output(conv_block_4/dropout_layer_2) shape: [1, 2, 2, 512]\n",
      "\tMaxPool2D layer output(conv_block_4/max_pool_layer_2) shape: [1, 2, 2, 512]\n",
      "\tConv2D layer output(conv_block_4/conv_layer_2) shape: [1, 4, 4, 512]\n",
      "\tConv2D layer output(conv_block_4/conv_layer_1) shape: [1, 4, 4, 512]\n",
      "\tConv2D layer output(conv_block_4/conv_layer_0) shape: [1, 4, 4, 512]\n",
      "conv_block_3:\n",
      "\tDropout layer output(conv_block_3/dropout_layer_2) shape: [1, 4, 4, 256]\n",
      "\tMaxPool2D layer output(conv_block_3/max_pool_layer_2) shape: [1, 4, 4, 256]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_2) shape: [1, 8, 8, 256]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_1) shape: [1, 8, 8, 256]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_0) shape: [1, 8, 8, 256]\n",
      "conv_block_2:\n",
      "\tDropout layer output(conv_block_2/dropout_layer_1) shape: [1, 8, 8, 128]\n",
      "\tMaxPool2D layer output(conv_block_2/max_pool_layer_1) shape: [1, 8, 8, 128]\n",
      "\tConv2D layer output(conv_block_2/conv_layer_1) shape: [1, 16, 16, 128]\n",
      "\tConv2D layer output(conv_block_2/conv_layer_0) shape: [1, 16, 16, 128]\n",
      "conv_block_1:\n",
      "\tDropout layer output(conv_block_1/dropout_layer_1) shape: [1, 16, 16, 64]\n",
      "\tMaxPool2D layer output(conv_block_1/max_pool_layer_1) shape: [1, 16, 16, 64]\n",
      "\tConv2D layer output(conv_block_1/conv_layer_1) shape: [1, 32, 32, 64]\n",
      "\tConv2D layer output(conv_block_1/conv_layer_0) shape: [1, 32, 32, 64]\n",
      "---------------------------------------------------------------------------\n",
      "Starting training for VGG16PlusPlus...\n",
      "Epoch 1/100: Train Loss: 2.3861, Val Loss: 2.3029, Val Acc: 0.0978, Time: 84.73s\n",
      "Epoch 2/100: Train Loss: 2.3028, Val Loss: 2.3026, Val Acc: 0.1012, Time: 81.55s\n",
      "Epoch 3/100: Train Loss: 2.3027, Val Loss: 2.3028, Val Acc: 0.0997, Time: 81.42s\n",
      "Epoch 4/100: Train Loss: 2.3027, Val Loss: 2.3027, Val Acc: 0.1012, Time: 81.31s\n",
      "Epoch 5/100: Train Loss: 2.3027, Val Loss: 2.3028, Val Acc: 0.0997, Time: 81.27s\n",
      "Current lr= 0.001 Updated lr= 0.0005\n",
      "Epoch 6/100: Train Loss: 2.3025, Val Loss: 2.3029, Val Acc: 0.0997, Time: 81.24s\n",
      "Epoch 7/100: Train Loss: 2.3026, Val Loss: 2.3028, Val Acc: 0.0951, Time: 81.28s\n",
      "Current lr= 0.0005 Updated lr= 0.00025\n",
      "Epoch 8/100: Train Loss: 2.3026, Val Loss: 2.3028, Val Acc: 0.0951, Time: 81.27s\n",
      "Epoch 9/100: Train Loss: 2.3026, Val Loss: 2.3028, Val Acc: 0.0951, Time: 81.36s\n",
      "Epoch 10/100: Train Loss: 2.3026, Val Loss: 2.3028, Val Acc: 0.0951, Time: 81.32s\n",
      "Epoch 11/100: Train Loss: 2.3026, Val Loss: 2.3027, Val Acc: 0.0951, Time: 81.39s\n",
      "Epoch 12/100: Train Loss: 2.3026, Val Loss: 2.3027, Val Acc: 0.0951, Time: 81.30s\n",
      "Epoch 13/100: Train Loss: 2.3026, Val Loss: 2.3028, Val Acc: 0.0951, Time: 81.25s\n",
      "Epoch 14/100: Train Loss: 2.3026, Val Loss: 2.3027, Val Acc: 0.0981, Time: 81.14s\n",
      "Epoch 15/100: Train Loss: 2.3026, Val Loss: 2.3027, Val Acc: 0.0981, Time: 81.18s\n",
      "Epoch 16/100: Train Loss: 2.3026, Val Loss: 2.3027, Val Acc: 0.0978, Time: 81.58s\n",
      "Finished training after 16 epochs!\n",
      "Training completed in 1304.61 seconds (21.74 minutes)\n",
      "VGG16PlusPlus Test Accuracy: 0.0999\n",
      "Saved results for VGG16PlusPlus\n"
     ]
    }
   ],
   "source": [
    "SEED = 1\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "results = []\n",
    "# all_models = [\"VGG16PlusPlus, VGG16PlusPlus\"]\n",
    "all_models = [\"VGG16Plus\", \"VGG16PlusPlus\"]\n",
    "# all_models = [\"VGG15PlusPlus\", \"VGG15PlusPlusOffOn\"]\n",
    "# get dataset with validation split\n",
    "x_train, y_train, x_val, y_val, x_test, y_test, classnames = datasets.get_dataset('cifar10', val_prop=0.2)\n",
    "\n",
    "# loop through each model to train\n",
    "for model_idx, model_type in enumerate(all_models):\n",
    "    # clear session to free memory\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    current_seed = SEED + model_idx\n",
    "    tf.random.set_seed(current_seed)\n",
    "    np.random.seed(current_seed)\n",
    "    \n",
    "    print(f\"\\n{'='*20} Training {model_type} {'='*20}\")\n",
    "    \n",
    "    if model_type == \"VGG4Plus\":\n",
    "        model = VGG4Plus(C=10, input_feats_shape=(32, 32, 3), wt_init='he')\n",
    "    elif model_type == \"VGG15\":\n",
    "        model = VGG15(C=10, input_feats_shape=(32, 32, 3), wt_init='he')\n",
    "    elif model_type == \"VGG15Plus\":\n",
    "        model = VGG15Plus(C=10, input_feats_shape=(32, 32, 3), wt_init='he')\n",
    "    elif model_type == \"VGG15PlusPlus\":\n",
    "        model = VGG15PlusPlus(C=10, input_feats_shape=(32, 32, 3), wt_init='he')\n",
    "    elif model_type == \"VGG15PlusPlus\":\n",
    "        model = VGG15PlusPlus(C=10, input_feats_shape=(32, 32, 3), wt_init='he')\n",
    "    elif model_type == \"VGG15PlusPlusOffOn\":\n",
    "        model = VGG15PlusPlusOffOn(C=10, input_feats_shape=(32, 32, 3), wt_init='he')\n",
    "    elif model_type == \"VGG16Plus\":\n",
    "        model = VGG16Plus(C=10, input_feats_shape=(32, 32, 3), wt_init=\"he\")\n",
    "        # print(\"hii!\")\n",
    "    elif model_type == \"VGG16PlusPlus\":\n",
    "        model = VGG16PlusPlus(C=10, input_feats_shape=(32, 32, 3), wt_init=\"he\")\n",
    "        # print(model)\n",
    "    \n",
    "    # compile with AdamW optimizer\n",
    "    print(model_type)\n",
    "    model.compile(optimizer='adamw')\n",
    "    \n",
    "    # train the model\n",
    "    start_time = time.time()\n",
    "    print(f\"Starting training for {model_type}...\")\n",
    "    \n",
    "    train_loss_hist, val_loss_hist, val_acc_hist, epochs = model.fit(\n",
    "        x_train, y_train, \n",
    "        x_val, y_val, \n",
    "        max_epochs=100,\n",
    "        patience=15,\n",
    "        lr_patience=4,\n",
    "        verbose=True,\n",
    "        lr_decay_factor=0.5,\n",
    "        lr_max_decays=12\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
    "    \n",
    "    # evaluate on test set\n",
    "    test_acc, test_loss = model.evaluate(x_test, y_test)\n",
    "    print(f\"{model_type} Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    result = {\n",
    "        'model_type': model_type,\n",
    "        'test_accuracy': test_acc,\n",
    "        'test_loss': test_loss,\n",
    "        'train_loss_history': train_loss_hist,\n",
    "        'val_loss_history': val_loss_hist,\n",
    "        'val_acc_history': val_acc_hist,\n",
    "        'epochs': epochs,\n",
    "        'training_time': training_time\n",
    "    }\n",
    "    results.append(result)\n",
    "    \n",
    "    # Save individual results in case notebook crashes\n",
    "    np.save(f\"{model_type}_results_10b.npy\", result)\n",
    "    print(f\"Saved results for {model_type}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexAndDerekNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/codex/alexnet-complete-architecture-dc3a9920cdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vgg_nets import VGG8OnOff, VGG8OnOffNoReduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\alexl\\miniconda3\\envs\\tf_new\\lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "Dense layer output(output) shape: [1, 10]\n",
      "dense_block:\n",
      "\tDropout layer output(dense_block/dropout_layer_0) shape: [1, 512]\n",
      "\tDense layer output(dense_block/dense_layer_0) shape: [1, 512]\n",
      "Flatten layer output(flatten) shape: [1, 4096]\n",
      "conv_block_3:\n",
      "\tMaxPool2D layer output(conv_block_3/max_pool_layer_1) shape: [1, 4, 4, 256]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_1) shape: [1, 8, 8, 256]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_0) shape: [1, 8, 8, 256]\n",
      "conv_block_2:\n",
      "\tMaxPool2D layer output(conv_block_2/max_pool_layer_1) shape: [1, 8, 8, 128]\n",
      "\tConv2D layer output(conv_block_2/conv_layer_1) shape: [1, 16, 16, 128]\n",
      "\tConv2D layer output(conv_block_2/conv_layer_0) shape: [1, 16, 16, 128]\n",
      "conv_block_1:\n",
      "\tMaxPool2D layer output(conv_block_1/max_pool_layer_1) shape: [1, 16, 16, 64]\n",
      "\tConv2D layer output(conv_block_1/conv_layer_1) shape: [1, 32, 32, 64]\n",
      "\tConv2D layer output(conv_block_1/conv_layer_0) shape: [1, 32, 32, 64]\n",
      "---------------------------------------------------------------------------\n",
      "Starting training with patience=4...\n",
      "Epoch 1/10000: Train Loss: 1.6549, Val Loss: 1.3382, Val Acc: 0.5297, Time: 46.09s\n",
      "Epoch 2/10000: Train Loss: 1.1008, Val Loss: 0.9198, Val Acc: 0.6789, Time: 41.89s\n",
      "Epoch 3/10000: Train Loss: 0.8390, Val Loss: 0.7416, Val Acc: 0.7402, Time: 36.28s\n",
      "Epoch 4/10000: Train Loss: 0.6889, Val Loss: 0.7180, Val Acc: 0.7519, Time: 41.99s\n",
      "Epoch 5/10000: Train Loss: 0.5893, Val Loss: 0.6764, Val Acc: 0.7702, Time: 41.45s\n",
      "Epoch 6/10000: Train Loss: 0.5249, Val Loss: 0.6502, Val Acc: 0.7855, Time: 28.32s\n",
      "Epoch 7/10000: Train Loss: 0.4708, Val Loss: 0.6490, Val Acc: 0.7842, Time: 24.15s\n",
      "Epoch 8/10000: Train Loss: 0.4146, Val Loss: 0.6349, Val Acc: 0.7875, Time: 24.52s\n",
      "Epoch 9/10000: Train Loss: 0.3808, Val Loss: 0.5813, Val Acc: 0.8037, Time: 29.27s\n",
      "Epoch 10/10000: Train Loss: 0.3497, Val Loss: 0.6023, Val Acc: 0.7994, Time: 24.21s\n",
      "Epoch 11/10000: Train Loss: 0.3341, Val Loss: 0.6018, Val Acc: 0.8009, Time: 24.87s\n",
      "Epoch 12/10000: Train Loss: 0.3175, Val Loss: 0.6309, Val Acc: 0.7977, Time: 24.16s\n",
      "Finished training after 12 epochs!\n",
      "387.20846247673035\n",
      "test_acc\n"
     ]
    }
   ],
   "source": [
    "# Set random seed and clear session\n",
    "tf.random.set_seed(0)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Create network - changed to VGG8\n",
    "model = VGG8(C=10, \n",
    "            input_feats_shape=x_train.shape[1:], \n",
    "            wt_init=\"he\",\n",
    "            reg=0.6,\n",
    "            conv_dropout=False)\n",
    "\n",
    "# Set loss function and compile model\n",
    "model.compile(loss='cross_entropy', optimizer=\"adamw\", lr=0.001)\n",
    "\n",
    "print(f\"Starting training with patience={4}...\")\n",
    "\n",
    "start_total = time.time()\n",
    "\n",
    "# Train the model\n",
    "train_loss_hist, val_loss_hist, val_acc_hist, e = model.fit(\n",
    "    x_train, y_train, x_val, y_val, patience=4)\n",
    "\n",
    "total_time = time.time() - start_total\n",
    "\n",
    "# Evaluate on test set\n",
    "test_acc, test_loss = model.evaluate(x_test, y_test, batch_sz=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Dense layer output(output) shape: [1, 10]\n",
      "dense_block:\n",
      "\tDropout layer output(dense_block/dropout_layer_0) shape: [1, 512]\n",
      "\tDense layer output(dense_block/dense_layer_0) shape: [1, 512]\n",
      "Flatten layer output(flatten) shape: [1, 4096]\n",
      "conv_block_3:\n",
      "\tDropout layer output(conv_block_3/dropout_layer_1) shape: [1, 4, 4, 256]\n",
      "\tMaxPool2D layer output(conv_block_3/max_pool_layer_1) shape: [1, 4, 4, 256]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_1) shape: [1, 8, 8, 256]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_0) shape: [1, 8, 8, 256]\n",
      "conv_block_2:\n",
      "\tDropout layer output(conv_block_2/dropout_layer_1) shape: [1, 8, 8, 128]\n",
      "\tMaxPool2D layer output(conv_block_2/max_pool_layer_1) shape: [1, 8, 8, 128]\n",
      "\tConv2D layer output(conv_block_2/conv_layer_1) shape: [1, 16, 16, 128]\n",
      "\tConv2D layer output(conv_block_2/conv_layer_0) shape: [1, 16, 16, 128]\n",
      "conv_block_1:\n",
      "\tDropout layer output(conv_block_1/dropout_layer_1) shape: [1, 16, 16, 64]\n",
      "\tMaxPool2D layer output(conv_block_1/max_pool_layer_1) shape: [1, 16, 16, 64]\n",
      "\tConv2D layer output(conv_block_1/conv_layer_1) shape: [1, 32, 32, 64]\n",
      "\tConv2D layer output(conv_block_1/conv_layer_0) shape: [1, 32, 32, 64]\n",
      "---------------------------------------------------------------------------\n",
      "Starting training with patience=4...\n",
      "Epoch 1/10000: Train Loss: 1.7046, Val Loss: 1.3009, Val Acc: 0.5329, Time: 27.32s\n",
      "Epoch 2/10000: Train Loss: 1.1701, Val Loss: 0.9820, Val Acc: 0.6561, Time: 25.35s\n",
      "Epoch 3/10000: Train Loss: 0.9218, Val Loss: 0.8017, Val Acc: 0.7202, Time: 25.34s\n",
      "Epoch 4/10000: Train Loss: 0.7674, Val Loss: 0.7868, Val Acc: 0.7262, Time: 25.41s\n",
      "Epoch 5/10000: Train Loss: 0.6733, Val Loss: 0.6710, Val Acc: 0.7682, Time: 25.81s\n",
      "Epoch 6/10000: Train Loss: 0.6017, Val Loss: 0.6403, Val Acc: 0.7791, Time: 25.87s\n",
      "Epoch 7/10000: Train Loss: 0.5501, Val Loss: 0.6240, Val Acc: 0.7910, Time: 26.11s\n",
      "Epoch 8/10000: Train Loss: 0.4992, Val Loss: 0.5812, Val Acc: 0.8028, Time: 25.77s\n",
      "Epoch 9/10000: Train Loss: 0.4640, Val Loss: 0.5723, Val Acc: 0.8083, Time: 25.77s\n",
      "Epoch 10/10000: Train Loss: 0.4289, Val Loss: 0.5597, Val Acc: 0.8071, Time: 25.81s\n",
      "Epoch 11/10000: Train Loss: 0.4160, Val Loss: 0.5573, Val Acc: 0.8127, Time: 25.83s\n",
      "Epoch 12/10000: Train Loss: 0.3862, Val Loss: 0.5542, Val Acc: 0.8147, Time: 25.89s\n",
      "Epoch 13/10000: Train Loss: 0.3683, Val Loss: 0.5555, Val Acc: 0.8202, Time: 25.91s\n",
      "Epoch 14/10000: Train Loss: 0.3496, Val Loss: 0.6098, Val Acc: 0.7960, Time: 25.92s\n",
      "Epoch 15/10000: Train Loss: 0.3506, Val Loss: 0.6140, Val Acc: 0.7978, Time: 26.34s\n",
      "Finished training after 15 epochs!\n"
     ]
    }
   ],
   "source": [
    "# Set random seed and clear session\n",
    "tf.random.set_seed(0)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Create network - changed to VGG8\n",
    "model = VGG8(C=10, \n",
    "            input_feats_shape=x_train.shape[1:], \n",
    "            wt_init=\"he\",\n",
    "            reg=0.6,\n",
    "            conv_dropout=True)\n",
    "\n",
    "# Set loss function and compile model\n",
    "model.compile(loss='cross_entropy', optimizer=\"adamw\", lr=0.001)\n",
    "\n",
    "print(f\"Starting training with patience={4}...\")\n",
    "\n",
    "start_total = time.time()\n",
    "\n",
    "# Train the model\n",
    "train_loss_hist, val_loss_hist, val_acc_hist, e = model.fit(\n",
    "    x_train, y_train, x_val, y_val, patience=4)\n",
    "\n",
    "total_time = time.time() - start_total\n",
    "\n",
    "# Evaluate on test set\n",
    "test_acc, test_loss = model.evaluate(x_test, y_test, batch_sz=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.7931690812110901\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test acc: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Dense layer output(output) shape: [1, 10]\n",
      "dense_block:\n",
      "\tDropout layer output(dense_block/dropout_layer_0) shape: [1, 512]\n",
      "\tDense layer output(dense_block/dense_layer_0) shape: [1, 512]\n",
      "Flatten layer output(flatten) shape: [1, 1024]\n",
      "conv_block_3:\n",
      "\tDropout layer output(conv_block_3/dropout_layer_1) shape: [1, 2, 2, 256]\n",
      "\tMaxPool2D layer output(conv_block_3/max_pool_layer_1) shape: [1, 2, 2, 256]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_1) shape: [1, 4, 4, 256]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_0) shape: [1, 4, 4, 256]\n",
      "conv_block_2:\n",
      "\tDropout layer output(conv_block_2/dropout_layer_1) shape: [1, 4, 4, 128]\n",
      "\tMaxPool2D layer output(conv_block_2/max_pool_layer_1) shape: [1, 4, 4, 128]\n",
      "\tConv2D layer output(conv_block_2/conv_layer_1) shape: [1, 8, 8, 128]\n",
      "\tConv2D layer output(conv_block_2/conv_layer_0) shape: [1, 8, 8, 128]\n",
      "conv_block_1:\n",
      "\tDropout layer output(conv_block_1/dropout_layer_1) shape: [1, 8, 8, 64]\n",
      "\tMaxPool2D layer output(conv_block_1/max_pool_layer_1) shape: [1, 8, 8, 64]\n",
      "\tConv2D layer output(conv_block_1/conv_layer_1) shape: [1, 16, 16, 64]\n",
      "\tConv2D layer output(conv_block_1/conv_layer_0) shape: [1, 16, 16, 64]\n",
      "MaxPool2D layer output(on_off_maxpool) shape: [1, 16, 16, 6]\n",
      "---------------------------------------------------------------------------\n",
      "Starting training with patience=10...\n",
      "Epoch 1/10000: Train Loss: 1.7324, Val Loss: 1.3350, Val Acc: 0.5151, Time: 11.08s\n",
      "Epoch 2/10000: Train Loss: 1.2605, Val Loss: 1.1516, Val Acc: 0.5909, Time: 9.45s\n",
      "Epoch 3/10000: Train Loss: 1.0771, Val Loss: 1.0516, Val Acc: 0.6214, Time: 9.52s\n",
      "Epoch 4/10000: Train Loss: 0.9405, Val Loss: 0.9171, Val Acc: 0.6792, Time: 9.37s\n",
      "Epoch 5/10000: Train Loss: 0.8420, Val Loss: 0.8966, Val Acc: 0.6875, Time: 9.36s\n",
      "Epoch 6/10000: Train Loss: 0.7681, Val Loss: 0.8298, Val Acc: 0.7150, Time: 9.47s\n",
      "Epoch 7/10000: Train Loss: 0.7052, Val Loss: 0.7725, Val Acc: 0.7326, Time: 9.45s\n",
      "Epoch 8/10000: Train Loss: 0.6467, Val Loss: 0.7854, Val Acc: 0.7340, Time: 9.45s\n",
      "Epoch 9/10000: Train Loss: 0.6104, Val Loss: 0.7653, Val Acc: 0.7415, Time: 9.44s\n",
      "Epoch 10/10000: Train Loss: 0.5656, Val Loss: 0.7644, Val Acc: 0.7444, Time: 9.49s\n",
      "Epoch 11/10000: Train Loss: 0.5483, Val Loss: 0.7658, Val Acc: 0.7411, Time: 9.42s\n",
      "Epoch 12/10000: Train Loss: 0.5270, Val Loss: 0.7550, Val Acc: 0.7446, Time: 9.45s\n",
      "Epoch 13/10000: Train Loss: 0.4934, Val Loss: 0.7565, Val Acc: 0.7552, Time: 9.41s\n",
      "Epoch 14/10000: Train Loss: 0.4674, Val Loss: 0.7648, Val Acc: 0.7416, Time: 9.39s\n",
      "Epoch 15/10000: Train Loss: 0.4555, Val Loss: 0.7698, Val Acc: 0.7558, Time: 9.41s\n",
      "Epoch 16/10000: Train Loss: 0.4507, Val Loss: 0.7753, Val Acc: 0.7485, Time: 9.34s\n",
      "Epoch 17/10000: Train Loss: 0.4307, Val Loss: 0.7928, Val Acc: 0.7414, Time: 9.33s\n",
      "Epoch 18/10000: Train Loss: 0.4214, Val Loss: 0.7534, Val Acc: 0.7571, Time: 9.46s\n",
      "Epoch 19/10000: Train Loss: 0.4239, Val Loss: 0.7553, Val Acc: 0.7546, Time: 9.41s\n",
      "Epoch 20/10000: Train Loss: 0.3903, Val Loss: 0.7211, Val Acc: 0.7606, Time: 9.31s\n",
      "Epoch 21/10000: Train Loss: 0.3960, Val Loss: 0.7513, Val Acc: 0.7558, Time: 9.34s\n",
      "Epoch 22/10000: Train Loss: 0.3900, Val Loss: 0.9171, Val Acc: 0.7214, Time: 9.33s\n",
      "Epoch 23/10000: Train Loss: 0.3843, Val Loss: 0.7785, Val Acc: 0.7533, Time: 9.34s\n",
      "Epoch 24/10000: Train Loss: 0.3634, Val Loss: 0.7677, Val Acc: 0.7521, Time: 9.35s\n",
      "Epoch 25/10000: Train Loss: 0.3676, Val Loss: 0.7320, Val Acc: 0.7667, Time: 9.34s\n",
      "Epoch 26/10000: Train Loss: 0.3646, Val Loss: 0.7890, Val Acc: 0.7493, Time: 9.34s\n",
      "Epoch 27/10000: Train Loss: 0.3619, Val Loss: 0.7618, Val Acc: 0.7597, Time: 9.33s\n",
      "Epoch 28/10000: Train Loss: 0.3473, Val Loss: 0.7764, Val Acc: 0.7570, Time: 9.34s\n",
      "Epoch 29/10000: Train Loss: 0.3544, Val Loss: 0.7303, Val Acc: 0.7604, Time: 9.31s\n",
      "Finished training after 29 epochs!\n",
      "274.02891397476196\n",
      "test_acc\n"
     ]
    }
   ],
   "source": [
    "# Set random seed and clear session\n",
    "tf.random.set_seed(0)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = VGG8OnOff(C=10, \n",
    "            input_feats_shape=x_train.shape[1:], \n",
    "            wt_init=\"he\",\n",
    "            reg=0.6,\n",
    "            conv_dropout=True)\n",
    "\n",
    "# Set loss function and compile model\n",
    "model.compile(loss='cross_entropy', optimizer=\"adamw\", lr=0.001)\n",
    "\n",
    "print(f\"Starting training with patience={10}...\")\n",
    "\n",
    "start_total = time.time()\n",
    "\n",
    "# Train the model\n",
    "train_loss_hist, val_loss_hist, val_acc_hist, e = model.fit(\n",
    "    x_train, y_train, x_val, y_val, patience=10)\n",
    "\n",
    "total_time = time.time() - start_total\n",
    "\n",
    "# Evaluate on test set\n",
    "test_acc, test_loss = model.evaluate(x_test, y_test, batch_sz=128)\n",
    "print(total_time)\n",
    "print(\"test_acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.7519030570983887\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test acc: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Dense layer output(output) shape: [1, 10]\n",
      "dense_block:\n",
      "\tDropout layer output(dense_block/dropout_layer_0) shape: [1, 512]\n",
      "\tDense layer output(dense_block/dense_layer_0) shape: [1, 512]\n",
      "Flatten layer output(flatten) shape: [1, 4096]\n",
      "conv_block_3:\n",
      "\tDropout layer output(conv_block_3/dropout_layer_1) shape: [1, 4, 4, 256]\n",
      "\tMaxPool2D layer output(conv_block_3/max_pool_layer_1) shape: [1, 4, 4, 256]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_1) shape: [1, 8, 8, 256]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_0) shape: [1, 8, 8, 256]\n",
      "conv_block_2:\n",
      "\tDropout layer output(conv_block_2/dropout_layer_1) shape: [1, 8, 8, 128]\n",
      "\tMaxPool2D layer output(conv_block_2/max_pool_layer_1) shape: [1, 8, 8, 128]\n",
      "\tConv2D layer output(conv_block_2/conv_layer_1) shape: [1, 16, 16, 128]\n",
      "\tConv2D layer output(conv_block_2/conv_layer_0) shape: [1, 16, 16, 128]\n",
      "conv_block_1:\n",
      "\tDropout layer output(conv_block_1/dropout_layer_1) shape: [1, 16, 16, 64]\n",
      "\tMaxPool2D layer output(conv_block_1/max_pool_layer_1) shape: [1, 16, 16, 64]\n",
      "\tConv2D layer output(conv_block_1/conv_layer_1) shape: [1, 32, 32, 64]\n",
      "\tConv2D layer output(conv_block_1/conv_layer_0) shape: [1, 32, 32, 64]\n",
      "MaxPool2D layer output(on_off_maxpool) shape: [1, 32, 32, 6]\n",
      "---------------------------------------------------------------------------\n",
      "Starting training with patience=10...\n",
      "Epoch 1/10000: Train Loss: 1.7258, Val Loss: 1.3168, Val Acc: 0.5170, Time: 27.47s\n",
      "Epoch 2/10000: Train Loss: 1.1618, Val Loss: 1.0386, Val Acc: 0.6375, Time: 25.73s\n",
      "Epoch 3/10000: Train Loss: 0.9165, Val Loss: 0.8307, Val Acc: 0.7113, Time: 26.04s\n",
      "Epoch 4/10000: Train Loss: 0.7714, Val Loss: 0.7739, Val Acc: 0.7285, Time: 25.79s\n",
      "Epoch 5/10000: Train Loss: 0.6843, Val Loss: 0.7104, Val Acc: 0.7607, Time: 25.73s\n",
      "Epoch 6/10000: Train Loss: 0.6108, Val Loss: 0.6830, Val Acc: 0.7642, Time: 26.15s\n",
      "Epoch 7/10000: Train Loss: 0.5547, Val Loss: 0.5957, Val Acc: 0.7940, Time: 26.30s\n",
      "Epoch 8/10000: Train Loss: 0.5049, Val Loss: 0.6011, Val Acc: 0.7964, Time: 26.10s\n",
      "Epoch 9/10000: Train Loss: 0.4726, Val Loss: 0.5686, Val Acc: 0.8032, Time: 25.86s\n",
      "Epoch 10/10000: Train Loss: 0.4364, Val Loss: 0.5834, Val Acc: 0.8009, Time: 25.88s\n",
      "Epoch 11/10000: Train Loss: 0.4223, Val Loss: 0.6205, Val Acc: 0.7978, Time: 25.70s\n",
      "Epoch 12/10000: Train Loss: 0.3999, Val Loss: 0.5692, Val Acc: 0.8116, Time: 25.70s\n",
      "Epoch 13/10000: Train Loss: 0.3691, Val Loss: 0.6098, Val Acc: 0.8086, Time: 25.72s\n",
      "Epoch 14/10000: Train Loss: 0.3608, Val Loss: 0.5478, Val Acc: 0.8163, Time: 25.75s\n",
      "Epoch 15/10000: Train Loss: 0.3579, Val Loss: 0.6160, Val Acc: 0.8051, Time: 25.71s\n",
      "Epoch 16/10000: Train Loss: 0.3364, Val Loss: 0.5878, Val Acc: 0.8066, Time: 25.74s\n",
      "Epoch 17/10000: Train Loss: 0.3301, Val Loss: 0.5706, Val Acc: 0.8114, Time: 25.67s\n",
      "Epoch 18/10000: Train Loss: 0.3112, Val Loss: 0.5860, Val Acc: 0.8139, Time: 25.72s\n",
      "Epoch 19/10000: Train Loss: 0.3158, Val Loss: 0.5707, Val Acc: 0.8153, Time: 25.88s\n",
      "Epoch 20/10000: Train Loss: 0.3030, Val Loss: 0.5610, Val Acc: 0.8123, Time: 25.75s\n",
      "Epoch 21/10000: Train Loss: 0.3231, Val Loss: 0.6080, Val Acc: 0.8050, Time: 25.78s\n",
      "Epoch 22/10000: Train Loss: 0.2955, Val Loss: 0.5861, Val Acc: 0.8150, Time: 25.73s\n",
      "Epoch 23/10000: Train Loss: 0.2896, Val Loss: 0.5760, Val Acc: 0.8120, Time: 25.75s\n",
      "Finished training after 23 epochs!\n",
      "595.6471469402313\n",
      "test_acc\n"
     ]
    }
   ],
   "source": [
    "# Set random seed and clear session\n",
    "tf.random.set_seed(0)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = VGG8OnOffNoReduction(C=10, \n",
    "            input_feats_shape=x_train.shape[1:], \n",
    "            wt_init=\"he\",\n",
    "            reg=0.6,\n",
    "            conv_dropout=True)\n",
    "\n",
    "# Set loss function and compile model\n",
    "model.compile(loss='cross_entropy', optimizer=\"adamw\", lr=0.001)\n",
    "\n",
    "print(f\"Starting training with patience={10}...\")\n",
    "\n",
    "start_total = time.time()\n",
    "\n",
    "# Train the model\n",
    "train_loss_hist, val_loss_hist, val_acc_hist, e = model.fit(\n",
    "    x_train, y_train, x_val, y_val, patience=10)\n",
    "\n",
    "total_time = time.time() - start_total\n",
    "\n",
    "# Evaluate on test set\n",
    "test_acc, test_loss = model.evaluate(x_test, y_test, batch_sz=128)\n",
    "print(total_time)\n",
    "print(\"test_acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.8079928159713745\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test acc: {test_acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
