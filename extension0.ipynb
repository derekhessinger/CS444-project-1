{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vgg16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.digitalocean.com/community/tutorials/popular-deep-learning-architectures-alexnet-vgg-googlenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(['seaborn-v0_8-colorblind', 'seaborn-v0_8-darkgrid'])\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "# Automatically reload your external source code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vgg_nets import VGG16Plus, VGG16PlusPlus\n",
    "from vgg_nets import VGG8\n",
    "import datasets\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data\n",
    "x_train, y_train, x_val, y_val, x_test, y_test, classnames = datasets.get_dataset('cifar10', val_prop=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Dense layer output(output) shape: [1, 10]\n",
      "dense_block:\n",
      "\tDropout layer output(dense_block/dropout_layer_0) shape: [1, 4096]\n",
      "\tDense layer output(dense_block/dense_layer_0) shape: [1, 4096]\n",
      "dense_block:\n",
      "\tDropout layer output(dense_block/dropout_layer_0) shape: [1, 4096]\n",
      "\tDense layer output(dense_block/dense_layer_0) shape: [1, 4096]\n",
      "Flatten layer output(flatten) shape: [1, 512]\n",
      "conv_block_5:\n",
      "\tMaxPool2D layer output(conv_block_5/max_pool_layer_2) shape: [1, 1, 1, 512]\n",
      "\tConv2D layer output(conv_block_5/conv_layer_2) shape: [1, 2, 2, 512]\n",
      "\tConv2D layer output(conv_block_5/conv_layer_1) shape: [1, 2, 2, 512]\n",
      "\tConv2D layer output(conv_block_5/conv_layer_0) shape: [1, 2, 2, 512]\n",
      "conv_block_4:\n",
      "\tMaxPool2D layer output(conv_block_4/max_pool_layer_2) shape: [1, 2, 2, 512]\n",
      "\tConv2D layer output(conv_block_4/conv_layer_2) shape: [1, 4, 4, 512]\n",
      "\tConv2D layer output(conv_block_4/conv_layer_1) shape: [1, 4, 4, 512]\n",
      "\tConv2D layer output(conv_block_4/conv_layer_0) shape: [1, 4, 4, 512]\n",
      "conv_block_3:\n",
      "\tMaxPool2D layer output(conv_block_3/max_pool_layer_2) shape: [1, 4, 4, 256]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_2) shape: [1, 8, 8, 256]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_1) shape: [1, 8, 8, 256]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_0) shape: [1, 8, 8, 256]\n",
      "conv_block_2:\n",
      "\tMaxPool2D layer output(conv_block_2/max_pool_layer_1) shape: [1, 8, 8, 128]\n",
      "\tConv2D layer output(conv_block_2/conv_layer_1) shape: [1, 16, 16, 128]\n",
      "\tConv2D layer output(conv_block_2/conv_layer_0) shape: [1, 16, 16, 128]\n",
      "conv_block_1:\n",
      "\tMaxPool2D layer output(conv_block_1/max_pool_layer_1) shape: [1, 16, 16, 64]\n",
      "\tConv2D layer output(conv_block_1/conv_layer_1) shape: [1, 32, 32, 64]\n",
      "\tConv2D layer output(conv_block_1/conv_layer_0) shape: [1, 32, 32, 64]\n",
      "---------------------------------------------------------------------------\n",
      "Starting training VGG16Plus with patience=15...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m start_total \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m train_loss_hist, val_loss_hist, val_acc_hist, epochs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m total_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_total\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Evaluate on test set\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\AlexL\\OneDrive\\Documents\\Classes\\CS444\\CS444-project-1\\network.py:452\u001b[0m, in \u001b[0;36mDeepNetwork.fit\u001b[1;34m(self, x, y, x_val, y_val, batch_size, max_epochs, val_every, verbose, patience, lr_patience, lr_decay_factor, lr_max_decays)\u001b[0m\n\u001b[0;32m    449\u001b[0m     y_batch \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mgather(y, batch_indices)\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;66;03m# Perform one training step (forward + backward) on this mini-batch\u001b[39;00m\n\u001b[1;32m--> 452\u001b[0m     loss_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m     epoch_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(loss_value)\n\u001b[0;32m    455\u001b[0m \u001b[38;5;66;03m# Average training loss for this epoch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\AlexL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\AlexL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\AlexL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\AlexL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AlexL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\AlexL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[1;32mc:\\Users\\AlexL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\AlexL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set random seed and clear session\n",
    "tf.random.set_seed(0)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Create VGG15 network\n",
    "model = VGG16Plus(C=10, \n",
    "             input_feats_shape=x_train.shape[1:], \n",
    "             wt_init=\"he\",\n",
    "             reg=0.6)  # Use regularization strength of 0.6\n",
    "\n",
    "# Set loss function and compile model with AdamW optimizer\n",
    "model.loss_name = 'cross_entropy'\n",
    "model.compile(loss='cross_entropy', optimizer='adamw', lr=0.001)\n",
    "\n",
    "print(f\"Starting training VGG16Plus with patience=15...\")\n",
    "\n",
    "# Track training time\n",
    "start_total = time.time()\n",
    "\n",
    "# Train the model\n",
    "train_loss_hist, val_loss_hist, val_acc_hist, epochs = model.fit(\n",
    "    x_train, y_train, x_val, y_val, patience=15)\n",
    "\n",
    "total_time = time.time() - start_total\n",
    "\n",
    "# Evaluate on test set\n",
    "test_acc, test_loss = model.evaluate(x_test, y_test, batch_sz=128)\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"\\nTraining completed for VGG16\")\n",
    "print(f\"Number of epochs trained: {epochs}\")\n",
    "print(f\"Total training time: {total_time:.2f} seconds\")\n",
    "print(f\"Average time per epoch: {total_time/epochs:.2f} seconds\")\n",
    "print(f\"Final test accuracy: {test_acc:.4f}\")\n",
    "print(f\"Final training loss: {train_loss_hist[-1]:.4f}\")\n",
    "print(f\"Final validation loss: {val_loss_hist[-1]:.4f}\")\n",
    "print(f\"Final validation accuracy: {val_acc_hist[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed and clear session\n",
    "tf.random.set_seed(0)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Create VGG15 network\n",
    "model = VGG15(C=10, \n",
    "             input_feats_shape=x_train.shape[1:], \n",
    "             wt_init=\"he\",\n",
    "             reg=0.6)  # Use regularization strength of 0.6\n",
    "\n",
    "# Set loss function and compile model with AdamW optimizer\n",
    "model.loss_name = 'cross_entropy'\n",
    "model.compile(loss='cross_entropy', optimizer='adamw', lr=0.001)\n",
    "\n",
    "print(f\"Starting training VGG15 with patience=4...\")\n",
    "\n",
    "# Track training time\n",
    "start_total = time.time()\n",
    "\n",
    "# Train the model\n",
    "train_loss_hist, val_loss_hist, val_acc_hist, epochs = model.fit(\n",
    "    x_train, y_train, x_val, y_val, patience=4)\n",
    "\n",
    "total_time = time.time() - start_total\n",
    "\n",
    "# Evaluate on test set\n",
    "test_acc, test_loss = model.evaluate(x_test, y_test, batch_sz=128)\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"\\nTraining completed for VGG15\")\n",
    "print(f\"Number of epochs trained: {epochs}\")\n",
    "print(f\"Total training time: {total_time:.2f} seconds\")\n",
    "print(f\"Average time per epoch: {total_time/epochs:.2f} seconds\")\n",
    "print(f\"Final test accuracy: {test_acc:.4f}\")\n",
    "print(f\"Final training loss: {train_loss_hist[-1]:.4f}\")\n",
    "print(f\"Final validation loss: {val_loss_hist[-1]:.4f}\")\n",
    "print(f\"Final validation accuracy: {val_acc_hist[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexAndDerekNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/codex/alexnet-complete-architecture-dc3a9920cdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vgg_nets import VGG8OnOff, VGG8OnOffNoReduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\alexl\\miniconda3\\envs\\tf_new\\lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "Dense layer output(output) shape: [1, 10]\n",
      "dense_block:\n",
      "\tDropout layer output(dense_block/dropout_layer_0) shape: [1, 512]\n",
      "\tDense layer output(dense_block/dense_layer_0) shape: [1, 512]\n",
      "Flatten layer output(flatten) shape: [1, 4096]\n",
      "conv_block_3:\n",
      "\tMaxPool2D layer output(conv_block_3/max_pool_layer_1) shape: [1, 4, 4, 256]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_1) shape: [1, 8, 8, 256]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_0) shape: [1, 8, 8, 256]\n",
      "conv_block_2:\n",
      "\tMaxPool2D layer output(conv_block_2/max_pool_layer_1) shape: [1, 8, 8, 128]\n",
      "\tConv2D layer output(conv_block_2/conv_layer_1) shape: [1, 16, 16, 128]\n",
      "\tConv2D layer output(conv_block_2/conv_layer_0) shape: [1, 16, 16, 128]\n",
      "conv_block_1:\n",
      "\tMaxPool2D layer output(conv_block_1/max_pool_layer_1) shape: [1, 16, 16, 64]\n",
      "\tConv2D layer output(conv_block_1/conv_layer_1) shape: [1, 32, 32, 64]\n",
      "\tConv2D layer output(conv_block_1/conv_layer_0) shape: [1, 32, 32, 64]\n",
      "---------------------------------------------------------------------------\n",
      "Starting training with patience=4...\n",
      "Epoch 1/10000: Train Loss: 1.6549, Val Loss: 1.3382, Val Acc: 0.5297, Time: 46.09s\n",
      "Epoch 2/10000: Train Loss: 1.1008, Val Loss: 0.9198, Val Acc: 0.6789, Time: 41.89s\n",
      "Epoch 3/10000: Train Loss: 0.8390, Val Loss: 0.7416, Val Acc: 0.7402, Time: 36.28s\n",
      "Epoch 4/10000: Train Loss: 0.6889, Val Loss: 0.7180, Val Acc: 0.7519, Time: 41.99s\n",
      "Epoch 5/10000: Train Loss: 0.5893, Val Loss: 0.6764, Val Acc: 0.7702, Time: 41.45s\n",
      "Epoch 6/10000: Train Loss: 0.5249, Val Loss: 0.6502, Val Acc: 0.7855, Time: 28.32s\n",
      "Epoch 7/10000: Train Loss: 0.4708, Val Loss: 0.6490, Val Acc: 0.7842, Time: 24.15s\n",
      "Epoch 8/10000: Train Loss: 0.4146, Val Loss: 0.6349, Val Acc: 0.7875, Time: 24.52s\n",
      "Epoch 9/10000: Train Loss: 0.3808, Val Loss: 0.5813, Val Acc: 0.8037, Time: 29.27s\n",
      "Epoch 10/10000: Train Loss: 0.3497, Val Loss: 0.6023, Val Acc: 0.7994, Time: 24.21s\n",
      "Epoch 11/10000: Train Loss: 0.3341, Val Loss: 0.6018, Val Acc: 0.8009, Time: 24.87s\n",
      "Epoch 12/10000: Train Loss: 0.3175, Val Loss: 0.6309, Val Acc: 0.7977, Time: 24.16s\n",
      "Finished training after 12 epochs!\n",
      "387.20846247673035\n",
      "test_acc\n"
     ]
    }
   ],
   "source": [
    "# Set random seed and clear session\n",
    "tf.random.set_seed(0)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Create network - changed to VGG8\n",
    "model = VGG8(C=10, \n",
    "            input_feats_shape=x_train.shape[1:], \n",
    "            wt_init=\"he\",\n",
    "            reg=0.6,\n",
    "            conv_dropout=False)\n",
    "\n",
    "# Set loss function and compile model\n",
    "model.compile(loss='cross_entropy', optimizer=\"adamw\", lr=0.001)\n",
    "\n",
    "print(f\"Starting training with patience={4}...\")\n",
    "\n",
    "start_total = time.time()\n",
    "\n",
    "# Train the model\n",
    "train_loss_hist, val_loss_hist, val_acc_hist, e = model.fit(\n",
    "    x_train, y_train, x_val, y_val, patience=4)\n",
    "\n",
    "total_time = time.time() - start_total\n",
    "\n",
    "# Evaluate on test set\n",
    "test_acc, test_loss = model.evaluate(x_test, y_test, batch_sz=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Dense layer output(output) shape: [1, 10]\n",
      "dense_block:\n",
      "\tDropout layer output(dense_block/dropout_layer_0) shape: [1, 512]\n",
      "\tDense layer output(dense_block/dense_layer_0) shape: [1, 512]\n",
      "Flatten layer output(flatten) shape: [1, 4096]\n",
      "conv_block_3:\n",
      "\tDropout layer output(conv_block_3/dropout_layer_1) shape: [1, 4, 4, 256]\n",
      "\tMaxPool2D layer output(conv_block_3/max_pool_layer_1) shape: [1, 4, 4, 256]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_1) shape: [1, 8, 8, 256]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_0) shape: [1, 8, 8, 256]\n",
      "conv_block_2:\n",
      "\tDropout layer output(conv_block_2/dropout_layer_1) shape: [1, 8, 8, 128]\n",
      "\tMaxPool2D layer output(conv_block_2/max_pool_layer_1) shape: [1, 8, 8, 128]\n",
      "\tConv2D layer output(conv_block_2/conv_layer_1) shape: [1, 16, 16, 128]\n",
      "\tConv2D layer output(conv_block_2/conv_layer_0) shape: [1, 16, 16, 128]\n",
      "conv_block_1:\n",
      "\tDropout layer output(conv_block_1/dropout_layer_1) shape: [1, 16, 16, 64]\n",
      "\tMaxPool2D layer output(conv_block_1/max_pool_layer_1) shape: [1, 16, 16, 64]\n",
      "\tConv2D layer output(conv_block_1/conv_layer_1) shape: [1, 32, 32, 64]\n",
      "\tConv2D layer output(conv_block_1/conv_layer_0) shape: [1, 32, 32, 64]\n",
      "---------------------------------------------------------------------------\n",
      "Starting training with patience=4...\n",
      "Epoch 1/10000: Train Loss: 1.7046, Val Loss: 1.3009, Val Acc: 0.5329, Time: 27.32s\n",
      "Epoch 2/10000: Train Loss: 1.1701, Val Loss: 0.9820, Val Acc: 0.6561, Time: 25.35s\n",
      "Epoch 3/10000: Train Loss: 0.9218, Val Loss: 0.8017, Val Acc: 0.7202, Time: 25.34s\n",
      "Epoch 4/10000: Train Loss: 0.7674, Val Loss: 0.7868, Val Acc: 0.7262, Time: 25.41s\n",
      "Epoch 5/10000: Train Loss: 0.6733, Val Loss: 0.6710, Val Acc: 0.7682, Time: 25.81s\n",
      "Epoch 6/10000: Train Loss: 0.6017, Val Loss: 0.6403, Val Acc: 0.7791, Time: 25.87s\n",
      "Epoch 7/10000: Train Loss: 0.5501, Val Loss: 0.6240, Val Acc: 0.7910, Time: 26.11s\n",
      "Epoch 8/10000: Train Loss: 0.4992, Val Loss: 0.5812, Val Acc: 0.8028, Time: 25.77s\n",
      "Epoch 9/10000: Train Loss: 0.4640, Val Loss: 0.5723, Val Acc: 0.8083, Time: 25.77s\n",
      "Epoch 10/10000: Train Loss: 0.4289, Val Loss: 0.5597, Val Acc: 0.8071, Time: 25.81s\n",
      "Epoch 11/10000: Train Loss: 0.4160, Val Loss: 0.5573, Val Acc: 0.8127, Time: 25.83s\n",
      "Epoch 12/10000: Train Loss: 0.3862, Val Loss: 0.5542, Val Acc: 0.8147, Time: 25.89s\n",
      "Epoch 13/10000: Train Loss: 0.3683, Val Loss: 0.5555, Val Acc: 0.8202, Time: 25.91s\n",
      "Epoch 14/10000: Train Loss: 0.3496, Val Loss: 0.6098, Val Acc: 0.7960, Time: 25.92s\n",
      "Epoch 15/10000: Train Loss: 0.3506, Val Loss: 0.6140, Val Acc: 0.7978, Time: 26.34s\n",
      "Finished training after 15 epochs!\n"
     ]
    }
   ],
   "source": [
    "# Set random seed and clear session\n",
    "tf.random.set_seed(0)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Create network - changed to VGG8\n",
    "model = VGG8(C=10, \n",
    "            input_feats_shape=x_train.shape[1:], \n",
    "            wt_init=\"he\",\n",
    "            reg=0.6,\n",
    "            conv_dropout=True)\n",
    "\n",
    "# Set loss function and compile model\n",
    "model.compile(loss='cross_entropy', optimizer=\"adamw\", lr=0.001)\n",
    "\n",
    "print(f\"Starting training with patience={4}...\")\n",
    "\n",
    "start_total = time.time()\n",
    "\n",
    "# Train the model\n",
    "train_loss_hist, val_loss_hist, val_acc_hist, e = model.fit(\n",
    "    x_train, y_train, x_val, y_val, patience=4)\n",
    "\n",
    "total_time = time.time() - start_total\n",
    "\n",
    "# Evaluate on test set\n",
    "test_acc, test_loss = model.evaluate(x_test, y_test, batch_sz=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.7931690812110901\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test acc: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Dense layer output(output) shape: [1, 10]\n",
      "dense_block:\n",
      "\tDropout layer output(dense_block/dropout_layer_0) shape: [1, 512]\n",
      "\tDense layer output(dense_block/dense_layer_0) shape: [1, 512]\n",
      "Flatten layer output(flatten) shape: [1, 1024]\n",
      "conv_block_3:\n",
      "\tDropout layer output(conv_block_3/dropout_layer_1) shape: [1, 2, 2, 256]\n",
      "\tMaxPool2D layer output(conv_block_3/max_pool_layer_1) shape: [1, 2, 2, 256]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_1) shape: [1, 4, 4, 256]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_0) shape: [1, 4, 4, 256]\n",
      "conv_block_2:\n",
      "\tDropout layer output(conv_block_2/dropout_layer_1) shape: [1, 4, 4, 128]\n",
      "\tMaxPool2D layer output(conv_block_2/max_pool_layer_1) shape: [1, 4, 4, 128]\n",
      "\tConv2D layer output(conv_block_2/conv_layer_1) shape: [1, 8, 8, 128]\n",
      "\tConv2D layer output(conv_block_2/conv_layer_0) shape: [1, 8, 8, 128]\n",
      "conv_block_1:\n",
      "\tDropout layer output(conv_block_1/dropout_layer_1) shape: [1, 8, 8, 64]\n",
      "\tMaxPool2D layer output(conv_block_1/max_pool_layer_1) shape: [1, 8, 8, 64]\n",
      "\tConv2D layer output(conv_block_1/conv_layer_1) shape: [1, 16, 16, 64]\n",
      "\tConv2D layer output(conv_block_1/conv_layer_0) shape: [1, 16, 16, 64]\n",
      "MaxPool2D layer output(on_off_maxpool) shape: [1, 16, 16, 6]\n",
      "---------------------------------------------------------------------------\n",
      "Starting training with patience=10...\n",
      "Epoch 1/10000: Train Loss: 1.7324, Val Loss: 1.3350, Val Acc: 0.5151, Time: 11.08s\n",
      "Epoch 2/10000: Train Loss: 1.2605, Val Loss: 1.1516, Val Acc: 0.5909, Time: 9.45s\n",
      "Epoch 3/10000: Train Loss: 1.0771, Val Loss: 1.0516, Val Acc: 0.6214, Time: 9.52s\n",
      "Epoch 4/10000: Train Loss: 0.9405, Val Loss: 0.9171, Val Acc: 0.6792, Time: 9.37s\n",
      "Epoch 5/10000: Train Loss: 0.8420, Val Loss: 0.8966, Val Acc: 0.6875, Time: 9.36s\n",
      "Epoch 6/10000: Train Loss: 0.7681, Val Loss: 0.8298, Val Acc: 0.7150, Time: 9.47s\n",
      "Epoch 7/10000: Train Loss: 0.7052, Val Loss: 0.7725, Val Acc: 0.7326, Time: 9.45s\n",
      "Epoch 8/10000: Train Loss: 0.6467, Val Loss: 0.7854, Val Acc: 0.7340, Time: 9.45s\n",
      "Epoch 9/10000: Train Loss: 0.6104, Val Loss: 0.7653, Val Acc: 0.7415, Time: 9.44s\n",
      "Epoch 10/10000: Train Loss: 0.5656, Val Loss: 0.7644, Val Acc: 0.7444, Time: 9.49s\n",
      "Epoch 11/10000: Train Loss: 0.5483, Val Loss: 0.7658, Val Acc: 0.7411, Time: 9.42s\n",
      "Epoch 12/10000: Train Loss: 0.5270, Val Loss: 0.7550, Val Acc: 0.7446, Time: 9.45s\n",
      "Epoch 13/10000: Train Loss: 0.4934, Val Loss: 0.7565, Val Acc: 0.7552, Time: 9.41s\n",
      "Epoch 14/10000: Train Loss: 0.4674, Val Loss: 0.7648, Val Acc: 0.7416, Time: 9.39s\n",
      "Epoch 15/10000: Train Loss: 0.4555, Val Loss: 0.7698, Val Acc: 0.7558, Time: 9.41s\n",
      "Epoch 16/10000: Train Loss: 0.4507, Val Loss: 0.7753, Val Acc: 0.7485, Time: 9.34s\n",
      "Epoch 17/10000: Train Loss: 0.4307, Val Loss: 0.7928, Val Acc: 0.7414, Time: 9.33s\n",
      "Epoch 18/10000: Train Loss: 0.4214, Val Loss: 0.7534, Val Acc: 0.7571, Time: 9.46s\n",
      "Epoch 19/10000: Train Loss: 0.4239, Val Loss: 0.7553, Val Acc: 0.7546, Time: 9.41s\n",
      "Epoch 20/10000: Train Loss: 0.3903, Val Loss: 0.7211, Val Acc: 0.7606, Time: 9.31s\n",
      "Epoch 21/10000: Train Loss: 0.3960, Val Loss: 0.7513, Val Acc: 0.7558, Time: 9.34s\n",
      "Epoch 22/10000: Train Loss: 0.3900, Val Loss: 0.9171, Val Acc: 0.7214, Time: 9.33s\n",
      "Epoch 23/10000: Train Loss: 0.3843, Val Loss: 0.7785, Val Acc: 0.7533, Time: 9.34s\n",
      "Epoch 24/10000: Train Loss: 0.3634, Val Loss: 0.7677, Val Acc: 0.7521, Time: 9.35s\n",
      "Epoch 25/10000: Train Loss: 0.3676, Val Loss: 0.7320, Val Acc: 0.7667, Time: 9.34s\n",
      "Epoch 26/10000: Train Loss: 0.3646, Val Loss: 0.7890, Val Acc: 0.7493, Time: 9.34s\n",
      "Epoch 27/10000: Train Loss: 0.3619, Val Loss: 0.7618, Val Acc: 0.7597, Time: 9.33s\n",
      "Epoch 28/10000: Train Loss: 0.3473, Val Loss: 0.7764, Val Acc: 0.7570, Time: 9.34s\n",
      "Epoch 29/10000: Train Loss: 0.3544, Val Loss: 0.7303, Val Acc: 0.7604, Time: 9.31s\n",
      "Finished training after 29 epochs!\n",
      "274.02891397476196\n",
      "test_acc\n"
     ]
    }
   ],
   "source": [
    "# Set random seed and clear session\n",
    "tf.random.set_seed(0)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = VGG8OnOff(C=10, \n",
    "            input_feats_shape=x_train.shape[1:], \n",
    "            wt_init=\"he\",\n",
    "            reg=0.6,\n",
    "            conv_dropout=True)\n",
    "\n",
    "# Set loss function and compile model\n",
    "model.compile(loss='cross_entropy', optimizer=\"adamw\", lr=0.001)\n",
    "\n",
    "print(f\"Starting training with patience={10}...\")\n",
    "\n",
    "start_total = time.time()\n",
    "\n",
    "# Train the model\n",
    "train_loss_hist, val_loss_hist, val_acc_hist, e = model.fit(\n",
    "    x_train, y_train, x_val, y_val, patience=10)\n",
    "\n",
    "total_time = time.time() - start_total\n",
    "\n",
    "# Evaluate on test set\n",
    "test_acc, test_loss = model.evaluate(x_test, y_test, batch_sz=128)\n",
    "print(total_time)\n",
    "print(\"test_acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.7519030570983887\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test acc: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Dense layer output(output) shape: [1, 10]\n",
      "dense_block:\n",
      "\tDropout layer output(dense_block/dropout_layer_0) shape: [1, 512]\n",
      "\tDense layer output(dense_block/dense_layer_0) shape: [1, 512]\n",
      "Flatten layer output(flatten) shape: [1, 4096]\n",
      "conv_block_3:\n",
      "\tDropout layer output(conv_block_3/dropout_layer_1) shape: [1, 4, 4, 256]\n",
      "\tMaxPool2D layer output(conv_block_3/max_pool_layer_1) shape: [1, 4, 4, 256]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_1) shape: [1, 8, 8, 256]\n",
      "\tConv2D layer output(conv_block_3/conv_layer_0) shape: [1, 8, 8, 256]\n",
      "conv_block_2:\n",
      "\tDropout layer output(conv_block_2/dropout_layer_1) shape: [1, 8, 8, 128]\n",
      "\tMaxPool2D layer output(conv_block_2/max_pool_layer_1) shape: [1, 8, 8, 128]\n",
      "\tConv2D layer output(conv_block_2/conv_layer_1) shape: [1, 16, 16, 128]\n",
      "\tConv2D layer output(conv_block_2/conv_layer_0) shape: [1, 16, 16, 128]\n",
      "conv_block_1:\n",
      "\tDropout layer output(conv_block_1/dropout_layer_1) shape: [1, 16, 16, 64]\n",
      "\tMaxPool2D layer output(conv_block_1/max_pool_layer_1) shape: [1, 16, 16, 64]\n",
      "\tConv2D layer output(conv_block_1/conv_layer_1) shape: [1, 32, 32, 64]\n",
      "\tConv2D layer output(conv_block_1/conv_layer_0) shape: [1, 32, 32, 64]\n",
      "MaxPool2D layer output(on_off_maxpool) shape: [1, 32, 32, 6]\n",
      "---------------------------------------------------------------------------\n",
      "Starting training with patience=10...\n",
      "Epoch 1/10000: Train Loss: 1.7258, Val Loss: 1.3168, Val Acc: 0.5170, Time: 27.47s\n",
      "Epoch 2/10000: Train Loss: 1.1618, Val Loss: 1.0386, Val Acc: 0.6375, Time: 25.73s\n",
      "Epoch 3/10000: Train Loss: 0.9165, Val Loss: 0.8307, Val Acc: 0.7113, Time: 26.04s\n",
      "Epoch 4/10000: Train Loss: 0.7714, Val Loss: 0.7739, Val Acc: 0.7285, Time: 25.79s\n",
      "Epoch 5/10000: Train Loss: 0.6843, Val Loss: 0.7104, Val Acc: 0.7607, Time: 25.73s\n",
      "Epoch 6/10000: Train Loss: 0.6108, Val Loss: 0.6830, Val Acc: 0.7642, Time: 26.15s\n",
      "Epoch 7/10000: Train Loss: 0.5547, Val Loss: 0.5957, Val Acc: 0.7940, Time: 26.30s\n",
      "Epoch 8/10000: Train Loss: 0.5049, Val Loss: 0.6011, Val Acc: 0.7964, Time: 26.10s\n",
      "Epoch 9/10000: Train Loss: 0.4726, Val Loss: 0.5686, Val Acc: 0.8032, Time: 25.86s\n",
      "Epoch 10/10000: Train Loss: 0.4364, Val Loss: 0.5834, Val Acc: 0.8009, Time: 25.88s\n",
      "Epoch 11/10000: Train Loss: 0.4223, Val Loss: 0.6205, Val Acc: 0.7978, Time: 25.70s\n",
      "Epoch 12/10000: Train Loss: 0.3999, Val Loss: 0.5692, Val Acc: 0.8116, Time: 25.70s\n",
      "Epoch 13/10000: Train Loss: 0.3691, Val Loss: 0.6098, Val Acc: 0.8086, Time: 25.72s\n",
      "Epoch 14/10000: Train Loss: 0.3608, Val Loss: 0.5478, Val Acc: 0.8163, Time: 25.75s\n",
      "Epoch 15/10000: Train Loss: 0.3579, Val Loss: 0.6160, Val Acc: 0.8051, Time: 25.71s\n",
      "Epoch 16/10000: Train Loss: 0.3364, Val Loss: 0.5878, Val Acc: 0.8066, Time: 25.74s\n",
      "Epoch 17/10000: Train Loss: 0.3301, Val Loss: 0.5706, Val Acc: 0.8114, Time: 25.67s\n",
      "Epoch 18/10000: Train Loss: 0.3112, Val Loss: 0.5860, Val Acc: 0.8139, Time: 25.72s\n",
      "Epoch 19/10000: Train Loss: 0.3158, Val Loss: 0.5707, Val Acc: 0.8153, Time: 25.88s\n",
      "Epoch 20/10000: Train Loss: 0.3030, Val Loss: 0.5610, Val Acc: 0.8123, Time: 25.75s\n",
      "Epoch 21/10000: Train Loss: 0.3231, Val Loss: 0.6080, Val Acc: 0.8050, Time: 25.78s\n",
      "Epoch 22/10000: Train Loss: 0.2955, Val Loss: 0.5861, Val Acc: 0.8150, Time: 25.73s\n",
      "Epoch 23/10000: Train Loss: 0.2896, Val Loss: 0.5760, Val Acc: 0.8120, Time: 25.75s\n",
      "Finished training after 23 epochs!\n",
      "595.6471469402313\n",
      "test_acc\n"
     ]
    }
   ],
   "source": [
    "# Set random seed and clear session\n",
    "tf.random.set_seed(0)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = VGG8OnOffNoReduction(C=10, \n",
    "            input_feats_shape=x_train.shape[1:], \n",
    "            wt_init=\"he\",\n",
    "            reg=0.6,\n",
    "            conv_dropout=True)\n",
    "\n",
    "# Set loss function and compile model\n",
    "model.compile(loss='cross_entropy', optimizer=\"adamw\", lr=0.001)\n",
    "\n",
    "print(f\"Starting training with patience={10}...\")\n",
    "\n",
    "start_total = time.time()\n",
    "\n",
    "# Train the model\n",
    "train_loss_hist, val_loss_hist, val_acc_hist, e = model.fit(\n",
    "    x_train, y_train, x_val, y_val, patience=10)\n",
    "\n",
    "total_time = time.time() - start_total\n",
    "\n",
    "# Evaluate on test set\n",
    "test_acc, test_loss = model.evaluate(x_test, y_test, batch_sz=128)\n",
    "print(total_time)\n",
    "print(\"test_acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.8079928159713745\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test acc: {test_acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
